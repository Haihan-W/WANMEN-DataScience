{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ML Structure.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1.熟 悉Jupyter notebook及环境配置 (notes 1.1-1.17)\n",
    "\n",
    "* Notes 1.01-1.08 python 环境与版本\n",
    "* Notes 1.09-1.10 graphviz 决策树可视化工具\n",
    "* Notes 1.11-1.12 几个重要的工具：numpy, scipy, sklearn, pandas, keras, tensorflow\n",
    "* Notes 1.13-1.14 安装 tensorflow 与 keras\n",
    "* Notes 1.15-1.17 jupyter notebook基本使用 and markdown的用法\n",
    "* EXTRA NOTES 如果换电脑,如何快速将原有机器上的环境配置装到新机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 2.学习资源,搜索方法,文献与代码管理工具及统计基础(Notes 2.1-2.11)\n",
    "\n",
    "* Notes 2.01-2.10 学习资源,搜索方法,文献与代码管理工具\n",
    "    1. 好好上网 stackoverflow, ...\n",
    "    2. Mendeley\n",
    "    3. github\n",
    "    \n",
    "\n",
    "* Notes 2.11 统计基础\n",
    "    1. 均值\n",
    "    2. 方差\n",
    "    3. 大数定理\n",
    "    4. 中心极限定理\n",
    "    5. 假设检验\n",
    "    6. p-value定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3.Python Basic Data Type基本数据类型\n",
    "- Notes 3.01- 3.07 Python 概述(使用环境,基本电脑知识,程序员步骤)\n",
    "- Notes 3.08-3.17 Python and data type and their built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4.函数function与Python data structure 基本数据结构\n",
    "- Notes 4.01-4.05 Python User-defined Function\n",
    "    - Positional Arguments\n",
    "    - Keyword Arguments\n",
    "    - Gather ***Unlimited Number*** of Positional Arguments with `*`\n",
    "    - Gather ***Unlimited Number*** of Keyword Arguments with `**`\n",
    "    - Default Parameter Values\n",
    "    - Docstrings--给自定义function写注释documentation,就像function help一样\n",
    "    - Lambda function\n",
    "    - 函数的传递和嵌套\n",
    "    - Error Handler\n",
    "- Notes 4.06-4.08 Python Code Structure\n",
    "    - Code Structure: Sequence-- 顺序执行\n",
    "    - Code Structure: condition: If (Cont.)--条件执行\n",
    "    - Code Structure: Loop (Cont.)--循环执行\n",
    "    - Generator\n",
    "    - Decorator\n",
    "    - Nampespace & Scope-- Global variable and local variable(within function)\n",
    "    - Use of `__` in Names-- function.`__`name`__` AND function.`__`doc`__`\n",
    "- Notes 4.09 Python Modules and Packages and Programs\n",
    "- Notes 4.10-4.11 python+basic+data+structure\n",
    "    - Container Types: \n",
    "        - List\n",
    "        - Tuple\n",
    "        - Dictionary\n",
    "        - Set\n",
    "    - Container Type Conversion\n",
    "    - Mutable (changeable), Immutable data type/structure\n",
    "    - Traversal Sequence Data Structure\n",
    "    - Advanced topics of list-- List Comprehensions!\n",
    "- Notes 4.12 Advanced topics in Python-Object oriented class&func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 5.Numpy 基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 5.02 Create Arrays\n",
    "    - How to create an numpy array?\n",
    "    - Shape of array\n",
    "    - Axis of arrays\n",
    "    - Create Special Arrays\n",
    "- Notes 5.03 Basic Operations of Arrays\n",
    "    - Arithmetic Operators-- element-wise, do the operation for every elements in the array\n",
    "    - Matrix operations\n",
    "    - Other Operations\n",
    "- Notes 5.04-5.06 Indexing+Slicing+and+Iterating\n",
    "    - NOTE: VIP difference detween Numpy array slicing and Python list slicing\n",
    "    - One-dimensional arrays\n",
    "    - Multidimensional arrays\n",
    "    - Boolean Indexing\n",
    "- Notes 5.07-5.08 Matrix+Operations+II\n",
    "    - Matrix Transpose\n",
    "        - Method 1: .T\n",
    "        - Method 2: .transpose() --note the difference from .T\n",
    "    - Inverse Matrix\n",
    "        - Tell if 2 matrix are inverse matrix to each other\n",
    "    - Eigenvalue and Normalized Eigenvector\n",
    "    - Trace\n",
    "    - Reshape\n",
    "    - Swap axes\n",
    "    - Bincount -- (advanced Matrix Operation)\n",
    "- Notes 5.08-5.09 Universal functions\n",
    "    - arange(), exp(), sqrt(), random.randn(), shape(), reshape(), add()...\n",
    "- Notes 5.09-5.10 Array+processing\n",
    "    - Generate Grid\n",
    "        - Using Grid to bring to function for plotting purpose\n",
    "    - Numpy where function-- used to conditional merge two matrix or to modify existing matrix\n",
    "    - Some Statistical Processing - calculate sum, mean, st.dev, variance\n",
    "    - Array Sort\n",
    "    - sort along specific axis\n",
    "    - Test whether each element of a 1-D array is also present in a second array with np.in1d\n",
    "    - Get non-replicated elements in array with np.unique\n",
    "- Notes 5.11 Save and Load Array\n",
    "    - Saving array in binary format (.npy)- .save() and load array- .load()\n",
    "    - Saving multiple arrays into a zip file- .savez() and load separately\n",
    "    - Saving and loading into text(csv) files- .savetxt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 6. Pandas\n",
    "- Notes 6.01.Series\n",
    "    - VIP note: Pandas Series/Data Frame slicing mechanism is the same like Numpy-- can do Mutation by Slicing (except Boolean indexing) -- review Notes 5.04-5.06\n",
    "    - Def'n- Series=加了index(行号)的array-- series.values; series.index\n",
    "    - Properties\n",
    "    - Conversion between series and dict\n",
    "- Notes 6.02-6.05 DataFrame+Titanic+Example\n",
    "    - DataFrame -- like 2D series, has both row and col\n",
    "         - How to create one?\n",
    "         - Shape and Axis\n",
    "         - Method\n",
    "         - Modify DataFrame values-- change original dataframe-- .apply(lambda function)\n",
    "         - Assign to a new dataframe variable ( Not Change Original Dataframe)-- .assign(lambda function)\n",
    "    - Titanic example\n",
    "        - How to deal with missing value ?-- drop them or fill them with some value\n",
    "        - Basic Statistics\n",
    "- Notes 6.06 Index Objects- use datetime as index\n",
    "- Notes 6.07 Reindex\n",
    "    - Reindex for panda series\n",
    "    - Reindex for Panda DataFrame\n",
    "- Notes 6.08 Drop+Data\n",
    "    - Drop data in Series\n",
    "    - Drop data in DataFrame\n",
    "        - Drop function will not change original dataframe by default -- review Notes 6.02-6.05 Titanic Example\n",
    "        - If we want drop function to change original dataframe, use inplace=True!!!!!!\n",
    "- Notes 6.09-6.10 slice+data  \n",
    ">   - One thing importance *When slicing we are creating a VIEW so it will change the origin value*\n",
    "    - BUT 条件选取/Boolean indexing/iloc/loc 选取 不会change original value\n",
    "    - To avoid the confusion, simply use .copy() to create a indep copy, not a view, if we want to keep original thing not changed! (以上这几点与numpy array 一模一样)\n",
    "    - For dataframe, usually preferred method for slicing is to use iloc/loc, not linked chain--[ : ], [ , ]\n",
    "    \n",
    "    - Series slicing\n",
    "    - Panda Dataframe Slicing\n",
    "        - Linked Chain Method: 选取单列 (cannot use it to select 行)\n",
    "        - Linked Chain Method: 选取多列\n",
    "        - 条件选取\n",
    "        - loc or iloc 行列穿插选取-- this is the preferred method than 'linked chain' 法选取\n",
    "        - copy()-- Note: same as series and numpy array, slicing will change original dataframe, to avoid it, use .copy() to create a indep copy, not a view\n",
    "        - Add a new column\n",
    "        - assign new value to modify existing elements\n",
    "        - Boolean Indexing 选取\n",
    "        > Similar as array, by default, pass a boolean, it will select ROW(not column) when index=True!!!!!!(背)\n",
    "\n",
    "        >To apply to Column, just use .T()\n",
    "- Notes 6.11 Data+Alignment\n",
    "    - '+','-' and 'df1.add.(df2,fill_value=...)'\n",
    "- Notes 6.12 Rank and Sort\n",
    "    - .sort_index(), .sort_values(), .rank()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Matplotlib\n",
    "- Notes 7.01-7.05 Matplotlib\n",
    "    - simple plot\n",
    "    - Histogram (VIP)- bar plot\n",
    "    > used to describe category type of data\n",
    "        \n",
    "        - What is Patches?\n",
    "    - Scatter Plot (VIP)\n",
    "    - Pie chart\n",
    "    - Sub-plots\n",
    "    - Plot Axes-- main axis and inset axis (second, third, ... axis)\n",
    "    - Radar chart (not important)\n",
    "    - 3D plot\n",
    "    - Use Math Functions as label in graph?\n",
    "    - Import Images as plot?\n",
    "- Notes 7.06-7.08 Aggregation--'Group by' for dataframe\n",
    "    - Group by one key\n",
    "    - Group by two keys\n",
    "    - Count # of values -- .size( )\n",
    "    - how to iterate through each group? 怎么每个group 走一遍\n",
    "    - Group By the dict and series\n",
    "    - Groupbyobject.agg(userdefinedfunction) function-- use user-defined function to operate groupedby object (user defined function is similar like sum(), mean(), size() built-in functions)\n",
    "    - GroupbyObject.agg('stringfunction') same as GroupbyObject.stringfunction() --e.g. a.agg('sum') =a.sum()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8.什么是好的模型结果--Cost Function\n",
    "\n",
    "- Notes 8.01-8.09+ 8.11-8.13 什么是好的模型结果\n",
    "    - 连续变量的模型，如何来衡量模型结果？--Linear regression, etc.\n",
    "    - 非连续变量-分类解决\n",
    "        - 二分类\n",
    "            - 假设检验，p-value\n",
    "            - confusion matrix\n",
    "            - 召回率，准确率\n",
    "            - F1-score\n",
    "            - ROC & AUC\n",
    "        - 多分类模型--如何衡量模型结果？-- cross entropy (concept)\n",
    "    - imbalanced问题\n",
    "        - 重新采样(resampling)\n",
    "            - random under-sampling (E.g. 2)\n",
    "            - random over-sampling\n",
    "            - 基于cluster 的重新采样（先用聚类方法找出子类）\n",
    "            - 合成数据 SMOTE (E.g. 3)\n",
    "        - 集成算法 (Ensemble Method)-- in Chapter 13\n",
    "- Notes 8.10 F1-Score and ROC--For 二分类model\n",
    "    - 计算F1_Score-- AKA Balanced-F-score or F-measure\n",
    "    - 练习计算与画ROC\n",
    "        1. 生成原始数据\n",
    "        2. Generate model for classification\n",
    "        3. 用ROC进行model效果衡量\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9.Linear Regression\n",
    "- Notes 9.01-9.10 Linear_Regression_Theory\n",
    "    1. 为什么要使用线性回归\n",
    "    2. 如何计算线性回归？\n",
    "    3. 由最小二乘法选出的直线有没有用？-- sampling and modeling multiple times, get variance (distribution) of coeff beta_hat-- see last picture in this step\n",
    "        - 1.Population has big variance (e.g. noise*400), so in the second step fit model, get slope deviates from original population slope=0.002\n",
    "        - 2.Population has small variance (e.g. noise*20), so in the second step fit model, get slope close to original population slope=0.002\n",
    "        - 3.Put them together sampling multiple times towards both population with big and small noise, then get big variance on model slope for population with big noise and small ... for small noise\n",
    "        \n",
    "    4. 线性回归参数估计的含义-- hypo test for correlation (beta_hat)\n",
    "    5. 线性回归对数据的解释 -- R^2\n",
    "    6. 线性回归对样本及误差的要求和假设前提 - L.I.N.E.\n",
    "    7. 预测的confidence interval 和 prediction interval\n",
    "- Notes 9.11-9.15 Practice\n",
    "    1. 我们先使用sklearn工具包进行线性回归\n",
    "    2. 使用统计包 statsmodels (more professional in data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 10.Logistic Regression +Softmax\n",
    "- Notes 10.01-10.06 logistic regression-Binary-Classification Problem\n",
    "- Notes 10.07-10.11 Solve_LogReg (code example)\n",
    "    - sklearn tool kit method\n",
    "    - Gradient Descent method\n",
    "    - Newton Rhapson method\n",
    "- Notes 10.12-10.13 Softmax-- solving multi-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11.Overfitting\n",
    "- Overfitting and prevention method\n",
    "    1. Fitting, overfitting and underfitting\n",
    "    2. Methods to prevent overfitting\n",
    "        1. Cross Validation\n",
    "        2. Regularization- limit model complexity\n",
    "            1. L1 norm: Lasso\n",
    "            2. L2 norm: Ridge\n",
    "    3. Python implementation\n",
    "        1. Cross validation\n",
    "            - Method 1: use KFold\n",
    "            - Method 2: cross_val_score -- one line code tp calculate accuracy\n",
    "        2. Regularization\n",
    "            1. L2: Ridge\n",
    "            2. L1: Lasso\n",
    "    4. Pipeline\n",
    "    5. SSE,SSR,SST-- just a review\n",
    "    6. Bias Variance and Noise (HARD!!! theory)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12.Decision Tree and Random Forest\n",
    "- Notes 12.01-12.06DecisionTree_RandomForest\n",
    "    - impurity\n",
    "        - Use Entropy to measure \n",
    "        - Use Gini impurity to measure\n",
    "    - How to deal with overfitting\n",
    "    - How to improve decision tree: history-- DT-> Bagging-> Random Forest\n",
    "- Notes 12.07-12.11Decision_Tree_Example_Titianic\n",
    "    - Reading data\n",
    "    - Data Exploration and Cleaning\n",
    "    - Modeling\n",
    "        - Train/Test split\n",
    "        - Building the Model: \n",
    "            1. define model: e.g. using decision tree as model\n",
    "            2. fit\n",
    "            3. prediction using testing dataset splitted from train_test_split()\n",
    "    - Evaluation\n",
    "    - Tree Visualization\n",
    "    - Parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13.Pandas Operations on data and Ensemble Method\n",
    "- Notes 13.01 Pandas-Permutation\n",
    "    - np.random.permutation -> not put back sampling\n",
    "    - np.random.randint -> sampling with putting back(with replacement)\n",
    "    - then use df.take to sampling from df or sampling pool\n",
    "- Notes 13.01-02 Pandas-Combining DataFrames\n",
    "    - Combine two data frames, fill in the NaN value in series 1 with non-empty value in series 2 with the same index\n",
    "        - Method 1. np.where(); \n",
    "        - Method 2. pandas series.combine_first(series2)\n",
    "- Notes 13.02 Pandas-Mapping\n",
    "    - Add column to df with .map(col)\n",
    "        - Trick: To ignore upper/lower case, simply use str.title()\n",
    "- Notes 13.03 Pandas-Binning\n",
    "    - .cut(list,categories) to get **category object** with following contents\n",
    "        - what category each element in list falls into\n",
    "        - length of object=size of the list\n",
    "        - a list of category\n",
    "    - attributes of **category object**\n",
    "        - codes\n",
    "        - categories\n",
    "    - pd.value_counts(_category object_): return how many elements from list falls in each category\n",
    "    - Two bins with bin1=(min,avg] and bin2=(avg,max] (note: bin is category)\n",
    "- Notes 13.03(2) GroupBy using Dict and Series and len(row name)\n",
    "    - create a groupby object using above 3 method\n",
    "    - Same result using dict and series\n",
    "    - Attributes of groupby object: count(), sum()\n",
    "- Notes 13.03-13.05 GroupBy using Dataframe itself\n",
    "    - Groupby using dataframe itself(E.g. column(s) of dataframe) is based on groupby using series (see previous section notes)\n",
    "    - Attributes: .mean(), .size()\n",
    "    - Iterate through a groupby object by its key(s)\n",
    "    - Present a groupby object in a list format/ dict format\n",
    "- Notes 13.06-07 Merge\n",
    "    - Comparison:\n",
    "        - groupby: within one dataframe, splitting to different group by groupname\n",
    "        - mergeby: different dataframe, want to merge together by criteria\n",
    "    - Merge on one key\n",
    "    - Merge on multiple keys\n",
    "    - Merge on index\n",
    "        - Merge on single layer index\n",
    "        - Merge on multi layer index\n",
    "- Notes 13.07-08 Deal with Missing Value\n",
    "    - Method 1: drop NaN--> .dropna(...)\n",
    "    - Method 2: fill with another value--> .fillna(...)\n",
    "- Notes 13.08 Outliers\n",
    "    - col[condition] method to pick rows in one col which row's value satisfy condition\n",
    "    - df[(condition).any(1)] method to pick any satisfied col/rows satisfying condition\n",
    "- Notes 13.09 Pivoting\n",
    "    - df.pivot(...) to create a default pivot table. \n",
    "- Notes 13.09 Rename Index and column of dataframe\n",
    "    - Method 1: use .index.map() or .columns.map()\n",
    "        - .map only create a copy, not change original; And it does not have inplace attribute (e.g. inplace=Flase/True)\n",
    "    - Method 2: use .rename(index=..., columns=...)\n",
    "        - .rename default inplace=false, set it to True to change the original df.\n",
    "    - Method 3: use .rename(dictionary) to change specific column or index\n",
    "- Notes 13.10 Replace\n",
    "    - replace single value in values\n",
    "    - use list to replace a list of value.\n",
    "    - use Dictionary to replace specific value.\n",
    "- Notes 13.10-14 Bagging, Ada Boosting, Gradient Boosting.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14.Airbnb Data Analysis (Kaggle competition project)\n",
    "- Notes 14.01-02 Introduction\n",
    "- Notes 14.03-05 Airbnb_DataExploration\n",
    "    - Data Exploration: Observation and find some features\n",
    "    - Summary of some features/patterns of the given parameters(columns):\n",
    "         - date_account_created and timestamp_first_active converted to date time format and then analysis\n",
    "         - date_First_booking too many Null value, maybe drop from analysis\n",
    "         - Age needs to remove outliers\n",
    "         -  Unknown gender may be set as 0.5\n",
    "- Notes 14.05-09 Airbnb_FeatureEngineering\n",
    "    - Feature Engineering: Data cleaning process\n",
    "    - Summary:\n",
    "        - Clean session data\n",
    "        - Clean training/testing data set: (Using ONE HOT ENCODING(pd.get_dummies) for assigning values to categorical features-- see tfa_wd [pd.getdummies] for example)\n",
    "        - Manipulate date/time data- format, feature extraction from date/time data (one hot encoding)- Extract Yr,Month,Date,Weekday,Season\n",
    "        - Manipulate Age data\n",
    "        - Manipulate Categorical feature data (one hot encoding)\n",
    "        - Merge cleaned Training/Testing dataset with cleaned Session dataset\n",
    "        - Split cleaned training/testing dataset; Save files to csv.\n",
    "- Notes 14.10-12 Airbnb_modeling\n",
    "    - How to load/save models (pickle.load, pickle.dump)\n",
    "    - Load previously saved cleaned datasets\n",
    "    - Start modelling\n",
    "        - Scoring method: NDCG\n",
    "            - NDCG evaluation: i.e. search engine sorting strategy.\n",
    "            - Consider both: 1. search by relevant. 2. more relevant, higher score (see PDF in the current folder called \"NDCG.PDF\")\n",
    "            - Check NDCG function that someone created from Kaggle- ndcg_score to see how the score is calculated\n",
    "        - Model 1: Logistic Regression\n",
    "            - For binary classification: we used to use f1 score, precision, recall, auc score. Here for Airbnb we use the ### NDCG evaluation ###.\n",
    "            - Learning curve of logistic regression (performance in terms of #iterations and data size)\n",
    "            \n",
    "        - Model 2: Tree methods\n",
    "            - decision tree\n",
    "            - random forest\n",
    "            - Adaboosting\n",
    "            - Bagging\n",
    "            - ExtraTree\n",
    "            - Gradient Boosting\n",
    "        - Model 3: SVM\n",
    "        - Model 4: xgboost\n",
    "    - Model comparison\n",
    "    - Conclusion and tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 15.SVM (Support Vector Machine)--SVC(SV Classification)+SVR(SV Regression)\n",
    "- Notes 15.01-10 Intro to SVM\n",
    "    - Brief history and intro\n",
    "    - Some definitions and SVM theory:\n",
    "        - Margin\n",
    "        - SV - support vector\n",
    "        - Lagrange Multiplier method - 等式约束, 不等式约束\n",
    "        - Duality- primary problem and dual problem\n",
    "        - KKT condition\n",
    "        - Kernel function - transformation to different dimension\n",
    "        - Soft margin and Regularization\n",
    "        \n",
    "- Notes 15.10-13 SVM code\n",
    "    - 1 . SVC\n",
    "        - 1.1 Binary Classification\n",
    "            - Kernel function = Linear\n",
    "            - Kernel function = rbf\n",
    "            - Compare effect of Gamma (only rbf kernel) and C (1/Regularization coeff) to overfitting\n",
    "                - SVM 参数1: C -- how different C will contribute to overfitting\n",
    "                - SVM 参数2: (rbf--高斯核函数 kernel only) gamma-- how diff Gamma will contribute to overfitting\n",
    "            - Compare different Kernel Function performance to this case(linear, rbf, polynomial, sigmoid)\n",
    "        - 1.2 Multiclasses Classification\n",
    "            - OVR (one vs. rest)\n",
    "            - OVO (one vs. one)\n",
    "    - 2 . SVR\n",
    "        - compare performace (kernel = linear vs. rbf vs. polynomial)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16.NLP\n",
    "- Notes 16.01-10 NLP history and intro.pdf\n",
    "    - NLP history\n",
    "    - Language model (Markov process)\n",
    "        - Markov process (chain) def'n\n",
    "        - 1st order Markov process (belongs to Bigram language model)\n",
    "        - 2nd order Markov process (belongs to Trigram language model)\n",
    "        - How to evaluate language: \n",
    "            - MLE (maximum likelihood estimate)- i.e. conditional probability\n",
    "            - Evaluation standard: Perplexity (the smaller the better)\n",
    "        - Data Sparse Problem, how to fix?\n",
    "            - Method 1: Linear Interpolation Smoothing\n",
    "            - Method 2: Laplace Smoothing\n",
    "            \n",
    "    - Hidden Markov Process\n",
    "        - Used to solve: Sequence-to-sequence problem\n",
    "        - How to evaluate languate:\n",
    "            - MLE: transition probability, emission probability\n",
    "        - Data Sparse problem\n",
    "        - Decoding problem\n",
    "        - Viterbi algorithm\n",
    "        \n",
    "    - Deep learning\n",
    "        - Use Neural Language Model (RNN, POS)\n",
    "        \n",
    "- Notes 16.11-13 NLP-Markov Process (code)\n",
    "    - Data Cleaning:\n",
    "        - import data from hongloumeng.txt\n",
    "        - remove punctuation and english characters(only chinese characters left)\n",
    "        - Use jieba.cut() to cut sentence to phrase\n",
    "    - Language Model: n-grams\n",
    "    - Maximum Likehood Parameter Estimation\n",
    "    - Evaluate Language Model with Perplexity(迷惑度) with train/test dataset:\n",
    "        - Split Data into Train, Valid, Test\n",
    "        - Build Language Model on Training dataset\n",
    "        - Test Our Uni-, Bi-, Tri-gram Model on Testing dataset\n",
    "            - 1.Testing using general log probability to calculate perplexity (PDF P23-25)\n",
    "            - 2.Testing using log probability w/ Linear Interpolation Smoothing to calculate perplexity (PDF P26-27)\n",
    "            - 3.Testing using log probability w/ Laplace Smoothing to calculate perplexity (PDF P28)- homework\n",
    "\n",
    "\n",
    "- Notes 16.13-14 Hidden Markov Process (词性标注-- POS(part of speech) tagging)\n",
    "    - A toy example\n",
    "        \n",
    "    - Algorithm: PDF (P 32-38)\n",
    "    \n",
    "    - Use nltk(Natural Language Toolkit) to build Hidden Markov Model (optional, NLTK may have bug)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 17. Text Processing and Amazon comment NLP project\n",
    "- Notes 17.01-13 Text Processing and Amazon comment NLP Project\n",
    "    - 1.python文字处理基本操作回顾\n",
    "    - 2.ASCII,unicode解码与编码,utf-8\n",
    "    - 3.nltk工具包与特朗普的任职演讲\n",
    "    - 4.Amazon评论分析案例\n",
    "        - 4.1 读取文字\n",
    "        - 4.2 整理标签-labelling (score=y_value -- positive feedback=1, negative feedback=0)\n",
    "        - 4.3 清理文字并建语料库- (x_value=summary text in customer comments)\n",
    "            - Remove Punctuation\n",
    "            - Tokenization\n",
    "            - Remove Stopwords and build corpus\n",
    "            - Stemming(词干提取) and Lemmatization(词性还原)\n",
    "        - 4.4 建模\n",
    "            - TF-IDF Evaluation (to convert comment words to number that indicates the frequecy of the words)\n",
    "            - clf (classification with different models -- e.g. Logistic Regression, Random Forest)\n",
    "        - 4.5 模型结果\n",
    "            - Evaluation (f1 score, precision score, recall score, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 18. Foundations of Web\n",
    "- Notes 18.01-14. Foundations of Web\n",
    "    - Data classification\n",
    "        - Structure Data- e.g. Relation DB\n",
    "        - Semi-structure Data- e.g. Excel, JSON, XML, Parquet\n",
    "        - Unstructure Data- e.g. Web data(HTML)\n",
    "    - Introduction to Web\n",
    "        - OSI model\n",
    "        - Server, switcher\n",
    "        - OS (Linux, Windows, Mac)\n",
    "        - Webpage\n",
    "            - Static webpage - HTML, Javascript, CSS, ...\n",
    "            - Dynamic webpage - backend, DB,...\n",
    "        - URL, HTTP request, HTTP response\n",
    "    - Web 2.0 + Open API\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19. Web Crawler 1\n",
    "- Notes 19.01-08 Web Crawler Toolkits and static webpage web crawler example(Charity Watch Website)\n",
    "    - Toolkits:\n",
    "        - Requests\n",
    "        - Beautiful Soup\n",
    "        \n",
    "    - Example of Web crawler using static webpage-- Charity Watch\n",
    "        - Open HTML of a webpage\n",
    "        - Locate the position of data of interest in HTML\n",
    "        - Using toolkits to do web crawling\n",
    "        \n",
    "- Notes 19.09-14 Dynamic webpage Web crawler example -- Bilibili comments\n",
    "    - note: Since webpage's html changed a lot from when the course was recorded. I found **the following method is NO Longer Effective...**\n",
    "    - Since it is a Dynamic website, we need api to get additional information that .request cannot get\n",
    "        - **api = Data receiver/connector**, obtain the cid (comment id), timestamp of the comment\n",
    "    - Every season has a season id\n",
    "    - Every episode has a episode id -- avid\n",
    "    - Every episode's comment has a comment id -- cid\n",
    "    - But every episode's webpage can only show <3000 comment searching by cid;\n",
    "    - Additional historical comments were stored into somewhere that can be searched by both cid and timestamp\n",
    "        - How the timestamp works?-- when comment is full (e.g. size>3000), then it was stored into a place labelled with cid and timestamp; Then a empty new comment place was created with cid and new timestamp to store the later comments...\n",
    "        - How to find the API to find the information above? -- Every video webpage has an API address\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20. Web Crawler 2\n",
    "- Notes 20.01-09 Web Crawler Advanced\n",
    "    - Write Binary File\n",
    "        - binary file: data stored as binary mode, i.e. 0 or 1\n",
    "        - open(filename, 'wb') #writable-binary file mode\n",
    "    - Read Binary File\n",
    "        - open(filename, 'rb') #readable-binary file mode\n",
    "    - List Matching with glob()\n",
    "        - The glob() function matches the file or directory names by using Unix shell rules rather than the more complete regular expression.\n",
    "            - * matches everything\n",
    "            - ? matches a single character\n",
    "            - [abc] matches a character a, b or c\n",
    "            - [!abc] matches any character except a, b, or c\n",
    "    - Extract information from url using urlparse\n",
    "    - How to download a file(e.g. stream video) directly using web crawler\n",
    "    - Authentication and Authorization\n",
    "        - Cookie-based webpage design\n",
    "            - in cookie-based webpage, after user logged in, there is a cookie/cookies generated and stored in your local host(PC)\n",
    "            - Then when user did some other request after log in, cookie will be carried along with the requests. So server can tell this request is from a user who logged in\n",
    "            - Cookie will have some features to expire after some time, or to be invalid after changing IP, etc. to enhance the security\n",
    "            - Example cookie can be found by -- inspect-> application--> cookies\n",
    "        - Tokend-based webpage design\n",
    "            - The general concept behind a token-based authentication system is simple. Allow users to enter their username and password in order to obtain a token which allows them to fetch a specific resource - without using their username and password. Once their token has been obtained, the user can offer the token - which offers access to a specific resource for a time period - to the remote site.\n",
    "\n",
    "            - In other words: add one level of indirection for authentication -- instead of having to authenticate with username and password for each protected resource, the user authenticates that way once (within a session of limited duration), obtains a time-limited token in return, and uses that token for further authentication during the session.\n",
    "\n",
    "            - Advantages are many -- e.g., the user could pass the token, once they've obtained it, on to some other automated system which they're willing to trust for a limited time and a limited set of resources, but would not be willing to trust with their username and password (i.e., with every resource they're allowed to access, forevermore or at least until they change their password).\n",
    "\n",
    "            - E.g. Third party authentication, log in to baiduyun program using QQ\n",
    "            \n",
    "    - Optional FYI knowledges\n",
    "        - Programs and Processes\n",
    "        - Concurrency-- called threading in Python\n",
    "        - Advanced Web Crawling\n",
    "            - Extract/store useful information of current webpage\n",
    "            - Get/store every external link in the current webpage\n",
    "            - Go to each of the external link, and store useful information of child pages.\n",
    "            - Traverse using a algorithm called Graph\n",
    "            - Set up some rules to prevent duplicated url (child webpage linked back to its parent webpage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 21. Regular Expression\n",
    "- Notes 21.01- 08 Regular Expression\n",
    "    - What it can be used for?\n",
    "    - Reference material\n",
    "        - https://regex101.com/ online regular expression tester and debugger (recommended)\n",
    "        - https://regex101.com/ online regular expression tester and debugger (recommended)\n",
    "        - https://deerchao.net/tutorials/regex/regex.htm  (30 mins quick guide)\n",
    "    - Common Reg Exp special characters\n",
    "    - Basic Grammar:\n",
    "        - re.compile()\n",
    "        - re.search/match()\n",
    "        - re.match() vs. re.search()\n",
    "        - re.findall()  - output in a list/array\n",
    "        - re.finditer() - print line by line\n",
    "    - special characters:\n",
    "        - 1.r combined with \\\n",
    "        - 2.1 r combined with \"\\\\x\", where \"\\x\" is any special meaning characters defined in Regular Expression (E.g. \"Common Reg Exp special characters\" at beginning of the notes)\n",
    "        - 2.2 r combined with \"\\ (\" or \"\\ )\"\n",
    "        - 3.* vs. +\n",
    "            - Example 3 - use * with cautious\n",
    "        - 4.Combination of different special characters of regular expressions\n",
    "    - Using | to separate two condition A and B\n",
    "    - findall 的两种用法\n",
    "    - 后向引用-- 如何捕获叠词 (e.g. ABC ABC)-- using \\1\n",
    "    - 零宽断言 - search 以 xxx 开头/结尾 的词\n",
    "    - Real examples\n",
    "\n",
    "\n",
    "- Notes 21.08 Regular Expression Case study\n",
    "    - Ex 1. Extract Number and Convert to right format of phone number\n",
    "    - Ex 2. Check if the email address is in valid format\n",
    "    - Ex 3. match every word except word \"python\"\n",
    "    - Ex 4. Find repeated word that is within 5 words apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 22. Bayes Stats I and Spam email detection case study\n",
    "- Notes 22.01-14 Bayes Stats I and Spam email detection case study\n",
    "    - Conditional Probability and Bayes Formula\n",
    "    - Case study: Detection of Spam\n",
    "    - Case study: 3 doors problem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 23. Notes 23.01-12 DataIntegration from user info\n",
    "- Notes 23.01-12 Notes 23.01-12 DataIntegration from user info (Bilibili website user info)\n",
    "    - Store Comment ids from user comments to csv file\n",
    "    - Relationship between comment ids and user ids(mid) -- encoding mid to comment id\n",
    "    - How to decoding commend id to get user id(mid)\n",
    "        - Step 1: Convert mid (0 -> 240,000,000) to its comment id using encoding method above, store into dictionary\n",
    "        - Step 2: Match each comment id (365891 rows) in dictionary (240,000,000 comment id dictionary), find its corresponding user id (mid); Store into csv.\n",
    "    - Obtain user info using user id(mid), from website (Bilibili)\n",
    "        - Step 1: create url list using user id(mid)\n",
    "        - Step 2: create some functions to bypass website's anti-web-crawler function on website\n",
    "        - Step 3: set up dataframe to store user info\n",
    "        - Step 4: create getsource() function to request/extract user info from website using url, and store them into dataframe\n",
    "        - Step 5: Store extracted user info to csv\n",
    "\n",
    "- Notes 23.13-14 Random Forest Recap (did not provide notes, ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24. Bayes Stats II\n",
    "- Notes 24.01-13 Bayes Stats II\n",
    "    - Bayes Stats II mindset:\n",
    "        - Static mindset\n",
    "        - Dynamic mindset:\n",
    "            - Prior Distribution(probability)\n",
    "            - Posterior Distribution(probability)\n",
    "    - Iterations to update Posterior Probability:\n",
    "        - Definition of Conjugate Distribution\n",
    "        - Proof: 二项分布(binomial distribution)和Beta分布(Beta Distribution)是共轭分布\n",
    "        - Proof: 正态分布(normal distribution) 的后验分布依然是正态分布 (共轭)\n",
    "    - 美国海岸救援 is using Bayes to update Posterior Probability, to search people\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 25. BiliBili User Comment Analysis\n",
    "- Notes 25.01-14 BiliBili User Comment Analysis\n",
    "    - Jieba Tokenize Tool\n",
    "        - jieba.cut\n",
    "        - jieba.suggest_freq\n",
    "    - Preprocess:\n",
    "        - Remove Stopword\n",
    "        - Drop NA/Duplicates\n",
    "        - Tokenize\n",
    "    - TF-IDF, Textrank\n",
    "    - Build customized Wordcloud\n",
    "    - Time-wise analysis:\n",
    "        - 不同剧集的高频词\n",
    "        - 不同剧集的弹幕随时间分布\n",
    "        - 24小时弹幕分布\n",
    "        - 年内弹幕分布\n",
    "    - Merge UserInfo/Gender with Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 26. Clustering\n",
    "- Notes 25.01-13 Clustering Introduction and Algorithms\n",
    "    - Clustering vs. Classification:\n",
    "        - Clustering belongs to Unsupervised Learning\n",
    "        - Classification belongs to Supervised Learning\n",
    "    - Clustering Measurement Parameters -- Distance\n",
    "        - Minkowski 距离\n",
    "        - 欧式距离（Euclidean distance）\n",
    "        - 曼哈顿距离（Manhattan distance）\n",
    "        - 夹角余弦 (cos(x,y))\n",
    "        - 相关系数（Pearson correlation coefficient）\n",
    "    - Clustering Methods:\n",
    "        - 基于划分的聚类（partitioning based clustering）：\n",
    "            - Algorithm: K均值（K-means)\n",
    "            - Disadvantage: \n",
    "                - Needs to know Predefined K, i.e. how many clusters you want\n",
    "                - highly rely on initial guess point (initial K-Mean assignment)\n",
    "            - Solve Disadvantage: Optimize Initial K-Mean assignment --> Kmeans++\n",
    "        - 层次聚类 (hierarchical clustering): \n",
    "            - Algorithm: Agglomerative clustering\n",
    "            - Distance Calculation Methods -- Linkage:\n",
    "                - Single Linkage: use min-distance\n",
    "                - Complete Linkage: use max-distance\n",
    "                - Average Linkage: use avg-distance\n",
    "                - note: each distance can be calculation use any of \"Clustering Measurement Parameters -- Distance\" mentioned above\n",
    "            - This Agglomerative Clustering Algorithm can be visualized using Dendrogram\n",
    "            - Disadvantages:\n",
    "                - Still need to know how many predefined K, i.e. how many clusters you want\n",
    "        - 密度聚类（density based clustering）： \n",
    "            - Algorithm: DBSCAN  **- Recommended**\n",
    "            - Parameters: Ɛ, MinPts\n",
    "            - How to find optimal Ɛ --> K-dist graph (where k=MinPts)\n",
    "            - Advantanges:\n",
    "                - Don't need to know predefined K\n",
    "                - Can recognize noise.\n",
    "        - 基于模型的聚类（model based clustering）： \n",
    "            - Algorithm: 高斯混合模型（GMM- Gaussian Mixed Model）\n",
    "            - GMM is a Probability Generation Model:\n",
    "                - i.e. Every sample is not 100% into one cluster, it can have x% in cluster 1, y% in cluster 2, ...\n",
    "        - Affinity propagation (not covered)\n",
    "        - Spectral clustering (not covered)\n",
    "\n",
    "- Notes 26.13-15 Clustering Code\n",
    "    - Performance Test for Each Clustering Methods under different shape of sample points(scatter plot):\n",
    "        - Sample Points Shape:\n",
    "            - Circle Shape\n",
    "            - Moon Shape\n",
    "            - Blobs Shape\n",
    "            - Blobs with varied variances\n",
    "            - Anisotropy 各向异性的 shape\n",
    "            - Random Shape\n",
    "        - Tested Methods:\n",
    "            - Kmeans\n",
    "            - DBSCAN\n",
    "            - Agglomerative Clustering with Average Linkage\n",
    "            - Agglomerative Clustering with Complete Linkage\n",
    "            - Spectral Clustering\n",
    "            - GMM\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 27. Public Opinion Analysis in Social Media\n",
    "- Notes 27.01-27.13 Public Opinion Analysis in Social Media\n",
    "    - Trends -> Strategy -> TACTICS -> Tools\n",
    "    - Marketing Research (TA insight), BI/Media Dept. (Content Advisor), Sales Dept. (Sales Transaction), Trade Marketing (Sales Execution)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 28. Recommendation System\n",
    "- Notes 28.01-10 Recommendation System (PDF)\n",
    "    - Recommendation System Application\n",
    "    - Algorithms:\n",
    "        1. Popularity Based (e.g. Top 10 merchandise(item) in 2019 January)\n",
    "        2. Graphic Based (e.g. Social Media like LinkedIn and Facebook)\n",
    "        3. Content Based (e.g. HR Hire Pre-screen: Male, Job experience >2 yrs, ...)\n",
    "        4. Collaborative Filtering (e.g. based on User Activity Historical data Features and Merchandise features)\n",
    "            - Algorithm:\n",
    "                - search Historical data\n",
    "                - Calculate Similarity\n",
    "                - Put to similarity matrix\n",
    "                - Make Recommendation (User based vs. Item Based)\n",
    "            - Problem:\n",
    "                - Large number of users/merchandises will cause sparse similarity matrix\n",
    "            \n",
    "           4.1 Matrix Factorization:\n",
    "               - Solved Sparse Problem of Similarity Matrix\n",
    "               - Generates a set of Hidden Variables, which can help to find some other hidden features between user and merchandises\n",
    "               - Algorithm: ALS (Alternating Least Squares)\n",
    "           4.2 Time Drifting Data:\n",
    "               - Update the recommendation with the change of time\n",
    "           4.3 LSTM\n",
    "           \n",
    "           4.4 CNN:\n",
    "               - Use CNN extract graphic feature\n",
    "               - Combined with CF Similarity Matrix Factorization\n",
    "               \n",
    "    - Evaluate Recommendation System:\n",
    "       - Precision Rate\n",
    "       - Recall Rate\n",
    "       - Covered Rate\n",
    "       - Creativity\n",
    "       - Testing methods:\n",
    "           - online vs. offline\n",
    "\n",
    "    - Others:\n",
    "        - Data Collection: needs to update recommendation system as required\n",
    "        - Data Integration: Activity Data from User is valuable information to study and integrate.\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 29. The History of AI\n",
    "- Notes 29.01-11 The History of AI (PDF)\n",
    "    1. 达特茅斯会议与第一次AI大发展\n",
    "    2. 第一次AI寒冬\n",
    "    3. 复苏与第二次AI寒冬\n",
    "    4. 再次复苏与神经网翻身\n",
    "    5. 瞻仰大神\n",
    "    6. 今天的应用与对社会的影响\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 30. The Application of ML in Image Processing\n",
    "- Notes 30.01-10 The Application of ML in Image Processing (PDF)\n",
    "    1. What is Image Processing:\n",
    "            a. Digital Signal Processing\n",
    "            b. Image Recognition\n",
    "            c. Image Understanding\n",
    "    2. Difficulty of Image Processing\n",
    "    3. History of Image Processing\n",
    "    4. What is Machine Learning\n",
    "            a. ML Category:\n",
    "                Supervised vs. Unsupervised vs. Reinforced Learning (which kind of utilize Supervised learning, but does not belong to either supervised or unsupervised learning)\n",
    "            b. ML vs. Deeplearning vs. AI vs. Big Data\n",
    "            c. General Workflow of ML\n",
    "            d. ML Subject Areas: Software, Stats, ML area, Intelligence Application, etc.\n",
    "            e. ML Methodology: (5 Tribes)\n",
    "                symbolicism - e.g. Decision Tree, Random Forest, AdaBoost (Adaptive Boost)\n",
    "                Bayes - e.g. Naive Bayes, Markov, Hidden Markov\n",
    "                Connectionism - e.g. ANN (CNN, RNN, Deep Learning, LSTM, GRU)\n",
    "                Evolutionism - e.g. genetic algorithm\n",
    "                Analogizer - e.g. KNN, SVM\n",
    "    5. Reference books and magazines for Image Processing\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 31. Build Flappy Bird Game from Scratch using Pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 31.01-12 Pygame (see pygame ipynotebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 32. Python, Operating System and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 32.01,09-10 Python, Operating System and Control (PDF)\n",
    "- Notes 32.02 Data Storage and its Types\n",
    "- Notes 32.03-04 OpenCV basic\n",
    "- Notes 32.05-08 Database Intro and MySQL Intro\n",
    "- Notes 32.10-11 Ctypes Basic\n",
    "    - Purpose: Use C++ programming language and its Library in Python\n",
    "- Notes 32.12 directkeys (Python file)\n",
    "    - Purpose: Utilize ctype library to assign WASD input key from keyboard for gaming control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 33. Image Recognition and Open CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 33.01-12 ImageRecognition\n",
    "    - Open CV Basics:\n",
    "        - Read Image: cv2.imread(), cv2.imshow(), cv2.imwrite()\n",
    "        - cv2.waitKey\n",
    "        - Read Video: cv2.VideoCapture(), cv2.VideoWriter()\n",
    "        - Draw Image: 画线cv2.line() , 圆形cv2.cicle() ,矩形 cv2.rectangle() , 椭圆cv2.ellipse() , 文本cv2.putText()\n",
    "        - Add Text to Image: cv2.putText\n",
    "    - Open CV Image Manipulation Basics:\n",
    "        - Get Color Index of selected pixel\n",
    "        - Change Color Index of selected pixel\n",
    "        - Get Image Properties: e.g. shape, color channel (e.g. R,G,B,...)\n",
    "        - Split/Merge Image Color Channel\n",
    "        - Padding: cv2.copyMakeBorder()\n",
    "        \n",
    "    - Open CV Image Processing (Calculation):\n",
    "        - Image Addition: cv2.add(x,y), or numpy add x+y\n",
    "        - Image Blending\n",
    "        - Image Bitwise Calculation: AND: cv2.bitwise_and; OR: cv2.bitwise_or; NOT: cv2.bitwise_not; XOR: cv2.bitwise_xor\n",
    "        \n",
    "    - Open CV Image Processing Advanced:\n",
    "        - Changing ColorSpaces (e.g. BGR to RGB,...)\n",
    "        - Object Tracking\n",
    "        - Image Thresholding (Reduce Noise)\n",
    "            - Simple Threshoding: global thresholding\n",
    "            - Adaptive Thresholding\n",
    "        - Image Geometric Transformation\n",
    "            - Scaling\n",
    "            - Translation\n",
    "            - Rotation\n",
    "            - Perspective Transformation, e.g. croping image\n",
    "        - Image Smoothing/Bluring\n",
    "            - 2D Convolution Image Filtering\n",
    "            - Normal Bluring (cv2.blur)\n",
    "            - Gaussian filtering/blur\n",
    "            - Bilateral Filtering\n",
    "            - Median filtering/blur\n",
    "        - Image Morphological Transformation\n",
    "            - Erosion\n",
    "            - Dilation (i.e. reverse of erosion)\n",
    "            - Open and Close calculation\n",
    "        - Gradient (for image sharpening)\n",
    "            - Sobel\n",
    "            - Scharr\n",
    "            - Laplacian\n",
    "        - Fourier Transform (for OCR, Noise Removal for Object Identification...)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 34. Feature Extraction from Game Data_GTA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 34.01-02 Snapshot.py\n",
    "    - Purpose of this python code: To snapshot a 800*600 size (or user defined size) of screenshot from topleft of the current window and save it as an img object\n",
    "    \n",
    "- Notes 34.03 Training_run.py\n",
    "    - Purpose of this code: Utilize snapshot .py and getkeys .py, store snapshot and its corresponding keyboard inputs to np format data file\n",
    "    \n",
    "- Notes 34.03-05 check data and OpenCV\n",
    "    - Check data generated from training_run.py above\n",
    "    - Using OpenCV to deal with Image data that is generated, e.g. Extract Lanes on the road. (e.g. color extraction, croping image...)\n",
    "    \n",
    "- Notes 34.06-13 GTA V Plug-In (C++) Intro and Code Review (Demo Only, did not go through code in details)\n",
    "    - GTA V Plug In Intro (34.06)\n",
    "    - C++ Review (34.07-08)\n",
    "    - Code Review - DeepGTAV PlugIn (34.09-11)\n",
    "    - Data Receiver - Using python to receive the data fetched from plugin while running the game (34.12-13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 35. GTA Auto Pilot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 35.01-06 GTA Project Breakdown - Divide and Conquer and Game Control\n",
    "- Note 35.07-13 GTA Road Lane_extraction\n",
    "    - Read and display image\n",
    "    - Edge Detection\n",
    "    - Color Extraction\n",
    "    - Lane Extraction - Find Main Lanes on the road\n",
    "        - Method: Clustering lines with similar slope and intersection to one group and calculate the average line representation of each group, to find the main group that contains large number of lines, and remove the noise group that contains small number of lines\n",
    "    - Game control i.e. how to make person in game to walk along the detected lane\n",
    "        - Code is in separate .py files which were not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 36. TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes 36.01-11,36.15 Introduction_to_tensorflow\n",
    "    - Tensor Shapes, Operations, Slicing, Sequences\n",
    "    - Tensor Graph: Visualization using Tensorboard\n",
    "    - Tensor Session: Session.run, Session.close, feed_dict...\n",
    "    - tf.constant: \n",
    "        - by using tf.InteractiveSession() -> we can use tf.constant(xxx).eval() to show constant value at jupyternotebook\n",
    "    - tf.Variable\n",
    "    - tf.placeholder\n",
    "    - How to organize Tensor Graph -> using name_scope\n",
    "    \n",
    "- Notes 36.12 Example 1_Building and Running your first TF Graph\n",
    "    - 1）用name scope 去划分 graph \n",
    "    - 2）写summary 为了可视化 \n",
    "    - 3）用sess 运行\n",
    "    \n",
    "- Notes 36.13-14 Example_2 Logistic Regression with TF\n",
    "    - Using Logistic Regression in TF to recognize the handwritten numbers (0~9) from MINST database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
