{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习1: 基本概念与简单例子\n",
    "\n",
    "* 1.复习监督学习\n",
    "* 2.强化学习系列课程基本概念与方法总览\n",
    "* 3.马尔可夫决策过程\n",
    "    * Markov Decision Processes\n",
    "\n",
    "* 4.Flappy bird的简单解决方法\n",
    "\n",
    "[参考视频David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "[参考书籍Sutton](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)\n",
    "\n",
    "[参考中文知乎](https://www.zhihu.com/people/flood-sung/activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.复习监督学习: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pic/Supervised.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare pred vs. label using cost function, to evaluate the model\n",
    "\n",
    "Cost function has different techniques/methods: e.g. for regression, using MSE; for categorization, using Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.强化学习系列课程基本概念与方法总览\n",
    "    * 2.1强化学习基本概念\n",
    "        * 特点：\n",
    "        1. 没有预先设定的label, but there is an overall goal (which can be quantified) -> the biggest difference between RL vs. Supervised Learning.\n",
    "        2. 反馈往往是滞后的:\n",
    "            reward caused by **current** action. (t=now)\n",
    "        3. 目标：累计奖励最大化:\n",
    "            Value=Reward (t=0) + Reward (t=1) + ......\n",
    "            Goal: Value is max overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept:\n",
    "> Policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_policy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Value: (overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_value.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 环境模型(model):\n",
    "    - a model which can define the environment accurately\n",
    "    - an environment can determine the current reward based on the action\n",
    "    - but most of time, we don't know the environment thus don't know the model\n",
    "    - therefore, trial-error method is used in RL to figure out the accurate environment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 个体agent\n",
    "    * 动作\n",
    "    * 总策略\n",
    "    * 例子：毕业后的选择，人生策略\n",
    "* 奖励\n",
    "    * 例子：毕业后的选择所赚的每年工资\n",
    "* 总价值\n",
    "    * $v_\\pi(S) = E_\\pi[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}...|S_t=s]$\n",
    "    * 例子：毕业\n",
    "* 总策略(policy)\n",
    "* 环境模型(model)\n",
    "    * 预判下一步状态以及概率\n",
    "        * $P_{ss^{'}}^{a}=P[S_{t+1}=s'|S_t=s,A_t=a]$\n",
    "    * 预判下一步的收益\n",
    "* 环境状态(state)\n",
    "* 强化学习：在不断地‘了解环境’与‘最大化已知环境下的总收益’\n",
    "    * learning vs planing \n",
    "    * exploring探索未知 vs exploitation利用已知(select the best current step's action from options)\n",
    "    * 选餐馆\n",
    "    * 选择钻井位置\n",
    "    * 选工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/Agent_Env.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2 策略与总价值 例子，走迷宫（选自David Silver讲义）\n",
    "<img src='pic/policy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/value.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 强化学习系列方法总览\n",
    "* Flappy bird的简单解决方法\n",
    "    * 如何衡量总价值\n",
    "    * 如何选择动作，选择总策略\n",
    "* 总价值不易计算时，但环境状态**有显式**的分布时\n",
    "    * 使用**迭代法**计算总价值(Value (overall))\n",
    "    * 使用**迭代法**反复改进总策略\n",
    "    * 策略迭代法的收敛\n",
    "* 总价值不易计算时，环境状态**没有显式**的分布时:\n",
    "    - method 1: 从连续的样本和经验中学习\n",
    "        * **蒙特卡洛**方法 (trial and error for every options)\n",
    "        * 计算总价值\n",
    "        * 更新总策略\n",
    "    - method 2: 从每一次与环境状态的交互中学习\n",
    "        * Temporal Differences\n",
    "            * SARSA (on policy)\n",
    "            * Q-learning (off policy):\n",
    "                - Q-learning still use MC to trial and error for every option on every step, but after every trial, it uses 迭代法 to update the Value (overall)\n",
    "        * Temporal Differences与蒙特卡罗方法的对比\n",
    "\n",
    "* 当**环境状态过多**，如何将有限样本中的策略推广到更大的状态空间，作为更大状态空间的近似解？\n",
    "    * Method 1: Combined with Supervised Learning:\n",
    "        - When there are too much environment states, it is not feasible to get the action for each state, we can sample limited amount of states, and RL on those to get actions; \n",
    "        - For the rest of states, using Supervised Learning (e.g. Decision Tree, SVM, Logi-Regression...) to predict the action given the state, based on those sampled amount of state-action pair obtained above.\n",
    "        <img src='pic/SupervisedLearning+RL.png'>\n",
    "    * Method 2: 线性方法等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Q-learing+Deep-Learning\n",
    "    * DQN:\n",
    "        - Example: Using CNN to extract feature of environment, output feature is Environment State; Then using RL\n",
    "    * DQN的优势与特点\n",
    "        <img src='./pic/DQN_Advantages.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.马尔可夫决策过程 Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Markov Property：现在决定未来\n",
    "- Definition: The Markov property means that evolution of the Markov process in the future (S_t+1) depends only on the present state (S_t) and does not depend on past history (S_1, S_2, ..., S_t-1). \n",
    "- AKA: **Memoryless Property**\n",
    "* $ P[S_{t+1}|S_t] = P[S_{t+1}|S_1,S_2,S_3,...,S_t]$\n",
    "\n",
    "\n",
    "### 3.2 Markov状态转移矩阵 - Markov State Transition Matrix\n",
    "- 状态转移概率：\n",
    "    - probabilities of transitioning from one state to another in a single time unit\n",
    "    - $ P_{ss^{'}}=P[S_{t+1}=s^{'}|S_t=s]$ -> the probability of state s goes to state s'\n",
    "* 状态转移矩阵：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{n,n} =\n",
    " \\begin{pmatrix}\n",
    "  p_{1,1} & p_{1,2} & \\cdots & p_{1,n} \\\\\n",
    "  p_{2,1} & p_{2,2} & \\cdots & p_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  p_{n,1} & p_{n,2} & \\cdots & p_{n,n}\n",
    " \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Markov State Transition Matrix.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 计算以下图示的状态转移矩阵：\n",
    "<img src='pic/mc_matrix.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> answer:\n",
    "\n",
    "\n",
    "<img src='pic/CalculateMarkovStateTransitionMatrix.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Markov Rewards Process\n",
    "* Definition: Current Reward Rs:\n",
    "    > Rs=E[Rt+1|St=s]\n",
    "* Future Reward: $R_{t+1}, R_{t+2}, ...$\n",
    "* Current Return 𝐺𝑡 is the total discounted reward from time-step t \n",
    "    > $ G_t = R_{t+1} + \\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$\n",
    "    - **$\\gamma$ is called \"discount-rate\" that reflecting the impact of time to when you get the reward.**\n",
    "    - for example: 100 CAD that you get today is different from 100 CAD that you get 10 years later.\n",
    "    - usually: 100 CAD that you get today worths more than 100 CAD that you get 10 years later because of the inflation of currency.\n",
    "    - therefore: $\\gamma$ is [0,1], meaning, the later you get the reward, the less valuable of that reward.\n",
    "* The value function v(s) gives the long-term value of state s:\n",
    "    > The state value function v(s) of an MRP is the **expected return 𝐺𝑡** ***starting from state s***\n",
    "    \n",
    "    > $v(s) = E[G_t|S_t=s]$, where E is Expectation (期望）， V is Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we manually define some Reward at each state, the reward is our 心情; Then the Overall Value V is to max out our 心情 overall\n",
    "\n",
    "<img src='pic/Rewards.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 状态价值state value\n",
    "* 从状态S=s 开始的总价值的期望值(as defined above)\n",
    "> $v(s) = E[G_t|S_t=s]$\n",
    "* 计算一下从 class1开始的马尔可夫过程的总价值，当$ \\gamma = 1/2$ ？\n",
    "    * C1,C2,C3,PASS,Sleep\n",
    "    * C1,FB,C1,C2,Pub\n",
    "    * C1,FB,FB,FB,FB\n",
    "* 分别计算以上路径的总价值，当$ \\gamma$ = 0\n",
    "* 分别计算以上路径的总价值，当$\\gamma$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Current状态价值可被分为两个部分，一是当前的奖励，二是下一步状态的总价值的贴现值\n",
    "\\begin{equation}\n",
    "v(s) = E[G_t|S_t=s]\\\\\n",
    "=E[R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... |S_t=s]\\\\\n",
    "=E[R_{t+1}|S_t=s]+\\gamma E[R_{t+2}+ \\gamma R_{t+3}+ ...|S_t=s]\\\\\n",
    "=E[R_{t+1}|S_t=s]+\\gamma E[G_{t+1}|S_t=s]\\\\\n",
    "\\end{equation}\n",
    "Please note: for $E[G_{t+1}|S_t=s], S_t=s$ means $S_{t+1}=s_{next}$, so $E[G_{t+1}|S_t=s]$ <=> $E[G_{t+1}|S_{t+1}=s_{next}] = v(s_{next})$\n",
    "\n",
    "\\begin{equation}\n",
    "v(s)=E[R_{t+1}|S_t=s]+\\gamma v(S_{next})|S_t=s\n",
    "\\end{equation}\n",
    "\n",
    "Also based on previous definition: Rs=E[Rt+1|St=s], we have:\n",
    "\n",
    "$v(s)=R_s+\\gamma v(S_{next})|S_t=s$\n",
    "\n",
    "For $v(S_{next})|S_t=s$ part, we can understand it as combination of all possible [next state's value] \\* [weight] (i.e. Probability of going to certain next state):\n",
    "> $\\sum_{s^{'}}P_{ss{'}}v(s^{'})$\n",
    "<img src='./pic/NextState_MarkovRewardProcess.png'>\n",
    "* $v(s) = R_s + \\gamma \\sum_{s^{'}}P_{ss{'}}v(s^{'})$\n",
    "\n",
    "- Using Matrix to express: (This is called **Bellman Equation**）\n",
    "> v(s) = R + $\\gamma$P v(s_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bellman方程\n",
    "* $v(s) = E[G_t|S_t=s]$  -> Bellman Equation for state-value\n",
    "* v(s) = R + $\\gamma$P v(s_next) -> Bellman Equation for state-value\n",
    "* P状态转移矩阵 -> Transition Probability (see 3.2)\n",
    "* 不动点求解:\n",
    "    - If it is in Steady State (equilibrium), all state will not change anymore with time, therefore s=s_next, \n",
    "    \n",
    "    <img src='pic/Bellman_Equation_SteadyState.png'>\n",
    "    - so we can get steady state v:\n",
    "    * v = $(I-\\gamma P)^{-1}R$\n",
    "    \n",
    "    \n",
    "* 这种直接求解只对小规模的MC有效\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例1: 一个棋盘只能：\n",
    "    * 上下左右走，各占1/4概率\n",
    "    * 不能走出界，否则棋子不动，并扣1分\n",
    "    * 移动到A,B处，各得10，5分，并在下一步移动至$A^{'},B^{'}$处\n",
    "    * $\\gamma=0.9$\n",
    "    - Question: What's the steady state's V(s)? (V(s) is a 5x5 matrix)\n",
    "<img src='pic/chess.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Bellman_Equation_Ex1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例2: 5个格子，最右的rewards为1，其余为0。左右均为终止状态。根据Bellman方程求v (at steady state)\n",
    "- note: action can be either to left or to right or stay(i.e. at 终止状态）， probability is 1/2 for each\n",
    "- assume  $\\gamma=1$\n",
    "<img src='pic/boxes.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Bellman_Equation_Ex2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 策略 (Policy) $\\pi$\n",
    "* $\\pi (a|s) = P[A_t=a|S_t=s]$ where \"a\" is action (e.g. move left, move right, stay still, ...)\n",
    "* 与时间无关，与状态有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some resource to help understand 3.7 and 3.8:\n",
    "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 状态价值函数(state-value) v.s 动作价值函数 (state-action-value)\n",
    "状态价值函数: (Bellman Expectation Equation for state-value)\n",
    "* $v_{\\pi}(s) = E_\\pi [G_t|S_t=s]$\n",
    "- we are finding the value of a particular state subjected to some policy(π), this is the difference between Bellman Equation (defined above) and the Bellman Expectation Equation(here).\n",
    "\n",
    "动作价值函数: (Bellman Expectation Equation for State-Action Value)\n",
    "* 动作价值函数多了一个动作a，代表在状态s下做了动作a之后的总价值期望\n",
    "* $q_{\\pi}(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$\n",
    "* 动作价值函数照样可以分解：\n",
    "    * $q_{\\pi}(s,a) = E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与当前的$v_{\\pi}(s)$的关系\n",
    "    - Usually in graphic expression: Hollow dot means State; Solid dot means Action\n",
    "    - This backup diagram describes the value of being in a particular state. From the state s there is some probability (pi(a|s)) that we take both the actions. There is a Q-value(State-action value function) for each of the action. We average the Q-values which tells us how good it is to be in a particular state. Basically, it defines Vπ(s).\n",
    "<img src='pic/v_q.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与**下一步**的$v_{\\pi}(s^{'})$的关系\n",
    "    - Tips: When t -> t+1, we need to add $R_s^a$ (see equation below); When t->t (no time step change), we don't need it (see equation above)\n",
    "<img src='pic/q_v.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$v_{\\pi}(s)$与**下一步**的$v_{\\pi}(s^{'})$的关系\n",
    "<img src='pic/v_v.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与**下一步**的$q_{\\pi}(s^{'},a^{'})$的关系\n",
    "<img src='pic/q_q.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, this is how we can formulate Bellman Expectation Equation for a given MDP to find it’s State-Value Function and State-Action Value Function. \n",
    "\n",
    "But, it does not tell us the best way to behave in an MDP. For that let’s talk about what is meant by Optimal Value and Optimal Policy Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 最优总价值(Optimal State-Value Function)与最优动作价值(Optimal State-Action Value Function (Q-Function))\n",
    "* Optimal State-Value Function:\n",
    "    - It is the maximum Value function over all policies.\n",
    "> $v_{*}(s)=max_{\\pi}v_{\\pi}(s)$\n",
    "* Optimal State-Action Value Function (Q-Function):\n",
    "    - It is the maximum action-value function over all policies.\n",
    "> $q_{*}(s,a)=max_{\\pi}q_{\\pi}(s,a)$\n",
    "* 对于任何MDP，必定有最优总价值与最优动作价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-v 与 当前max-q的关系\n",
    "    - note: only pick one action a, that can max out the q -> q*, therefore all the other actions has 0 probability ($\\pi$)\n",
    "<img src='pic/max_v.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called: **Bellman Optimality Equation for State-value Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-q 与 下一步max-v的关系\n",
    "<img src='pic/max_q.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Action Value Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-v 与 下一步max-v的关系\n",
    "<img src='pic/max_v_v.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Value Function from the Backup Diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-q 与 下一步max-q的关系\n",
    "<img src='pic/max_q_q.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Action Value Function from the Backup Diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一般来说最优状态价值或者最优动作价值都无法直接求, reason:\n",
    "    * need to 掌握环境的动态变化\n",
    "    * need 足够的运算能力\n",
    "    * need to 满足Markov性质\n",
    "* 不能直接求的，需要用到近似方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Flappy bird的简单解决方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Instead of using DQN (Image->CNN->Feature->RL), we can simplify the problem by manually defining/providing the feature to RL (i.e. Feature -> RL)\n",
    "- Advantage:\n",
    "    - Manual Feature Input will save lots of time of CNN auto detect the feature to use. (Simplified Method: few hundreds training time; DQN Method: Millions training time)\n",
    "- Disadvantage:\n",
    "    - Manual Feature method cannot be applied generally to other problem (e.g. other games), while DQN method can be applied to other games because we don't manually provide the feature to RL, CNN figured out features on its own, we only need to provide pictures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
