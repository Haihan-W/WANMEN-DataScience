{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼ºåŒ–å­¦ä¹ 1: åŸºæœ¬æ¦‚å¿µä¸ç®€å•ä¾‹å­\n",
    "\n",
    "* 1.å¤ä¹ ç›‘ç£å­¦ä¹ \n",
    "* 2.å¼ºåŒ–å­¦ä¹ ç³»åˆ—è¯¾ç¨‹åŸºæœ¬æ¦‚å¿µä¸æ–¹æ³•æ€»è§ˆ\n",
    "* 3.é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹\n",
    "    * Markov Decision Processes\n",
    "\n",
    "* 4.Flappy birdçš„ç®€å•è§£å†³æ–¹æ³•\n",
    "\n",
    "[å‚è€ƒè§†é¢‘David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "[å‚è€ƒä¹¦ç±Sutton](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)\n",
    "\n",
    "[å‚è€ƒä¸­æ–‡çŸ¥ä¹](https://www.zhihu.com/people/flood-sung/activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.å¤ä¹ ç›‘ç£å­¦ä¹ : Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pic/Supervised.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare pred vs. label using cost function, to evaluate the model\n",
    "\n",
    "Cost function has different techniques/methods: e.g. for regression, using MSE; for categorization, using Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.å¼ºåŒ–å­¦ä¹ ç³»åˆ—è¯¾ç¨‹åŸºæœ¬æ¦‚å¿µä¸æ–¹æ³•æ€»è§ˆ\n",
    "    * 2.1å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ\n",
    "        * ç‰¹ç‚¹ï¼š\n",
    "        1. æ²¡æœ‰é¢„å…ˆè®¾å®šçš„label, but there is an overall goal (which can be quantified) -> the biggest difference between RL vs. Supervised Learning.\n",
    "        2. åé¦ˆå¾€å¾€æ˜¯æ»åçš„:\n",
    "            reward caused by **current** action. (t=now)\n",
    "        3. ç›®æ ‡ï¼šç´¯è®¡å¥–åŠ±æœ€å¤§åŒ–:\n",
    "            Value=Reward (t=0) + Reward (t=1) + ......\n",
    "            Goal: Value is max overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept:\n",
    "> Policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_policy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Value: (overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/RL_value.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ç¯å¢ƒæ¨¡å‹(model):\n",
    "    - a model which can define the environment accurately\n",
    "    - an environment can determine the current reward based on the action\n",
    "    - but most of time, we don't know the environment thus don't know the model\n",
    "    - therefore, trial-error method is used in RL to figure out the accurate environment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä¸ªä½“agent\n",
    "    * åŠ¨ä½œ\n",
    "    * æ€»ç­–ç•¥\n",
    "    * ä¾‹å­ï¼šæ¯•ä¸šåçš„é€‰æ‹©ï¼Œäººç”Ÿç­–ç•¥\n",
    "* å¥–åŠ±\n",
    "    * ä¾‹å­ï¼šæ¯•ä¸šåçš„é€‰æ‹©æ‰€èµšçš„æ¯å¹´å·¥èµ„\n",
    "* æ€»ä»·å€¼\n",
    "    * $v_\\pi(S) = E_\\pi[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}...|S_t=s]$\n",
    "    * ä¾‹å­ï¼šæ¯•ä¸š\n",
    "* æ€»ç­–ç•¥(policy)\n",
    "* ç¯å¢ƒæ¨¡å‹(model)\n",
    "    * é¢„åˆ¤ä¸‹ä¸€æ­¥çŠ¶æ€ä»¥åŠæ¦‚ç‡\n",
    "        * $P_{ss^{'}}^{a}=P[S_{t+1}=s'|S_t=s,A_t=a]$\n",
    "    * é¢„åˆ¤ä¸‹ä¸€æ­¥çš„æ”¶ç›Š\n",
    "* ç¯å¢ƒçŠ¶æ€(state)\n",
    "* å¼ºåŒ–å­¦ä¹ ï¼šåœ¨ä¸æ–­åœ°â€˜äº†è§£ç¯å¢ƒâ€™ä¸â€˜æœ€å¤§åŒ–å·²çŸ¥ç¯å¢ƒä¸‹çš„æ€»æ”¶ç›Šâ€™\n",
    "    * learning vs planing \n",
    "    * exploringæ¢ç´¢æœªçŸ¥ vs exploitationåˆ©ç”¨å·²çŸ¥(select the best current step's action from options)\n",
    "    * é€‰é¤é¦†\n",
    "    * é€‰æ‹©é’»äº•ä½ç½®\n",
    "    * é€‰å·¥ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/Agent_Env.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2 ç­–ç•¥ä¸æ€»ä»·å€¼ ä¾‹å­ï¼Œèµ°è¿·å®«ï¼ˆé€‰è‡ªDavid Silverè®²ä¹‰ï¼‰\n",
    "<img src='pic/policy.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/value.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 å¼ºåŒ–å­¦ä¹ ç³»åˆ—æ–¹æ³•æ€»è§ˆ\n",
    "* Flappy birdçš„ç®€å•è§£å†³æ–¹æ³•\n",
    "    * å¦‚ä½•è¡¡é‡æ€»ä»·å€¼\n",
    "    * å¦‚ä½•é€‰æ‹©åŠ¨ä½œï¼Œé€‰æ‹©æ€»ç­–ç•¥\n",
    "* æ€»ä»·å€¼ä¸æ˜“è®¡ç®—æ—¶ï¼Œä½†ç¯å¢ƒçŠ¶æ€**æœ‰æ˜¾å¼**çš„åˆ†å¸ƒæ—¶\n",
    "    * ä½¿ç”¨**è¿­ä»£æ³•**è®¡ç®—æ€»ä»·å€¼(Value (overall))\n",
    "    * ä½¿ç”¨**è¿­ä»£æ³•**åå¤æ”¹è¿›æ€»ç­–ç•¥\n",
    "    * ç­–ç•¥è¿­ä»£æ³•çš„æ”¶æ•›\n",
    "* æ€»ä»·å€¼ä¸æ˜“è®¡ç®—æ—¶ï¼Œç¯å¢ƒçŠ¶æ€**æ²¡æœ‰æ˜¾å¼**çš„åˆ†å¸ƒæ—¶:\n",
    "    - method 1: ä»è¿ç»­çš„æ ·æœ¬å’Œç»éªŒä¸­å­¦ä¹ \n",
    "        * **è’™ç‰¹å¡æ´›**æ–¹æ³• (trial and error for every options)\n",
    "        * è®¡ç®—æ€»ä»·å€¼\n",
    "        * æ›´æ–°æ€»ç­–ç•¥\n",
    "    - method 2: ä»æ¯ä¸€æ¬¡ä¸ç¯å¢ƒçŠ¶æ€çš„äº¤äº’ä¸­å­¦ä¹ \n",
    "        * Temporal Differences\n",
    "            * SARSA (on policy)\n",
    "            * Q-learning (off policy):\n",
    "                - Q-learning still use MC to trial and error for every option on every step, but after every trial, it uses è¿­ä»£æ³• to update the Value (overall)\n",
    "        * Temporal Differencesä¸è’™ç‰¹å¡ç½—æ–¹æ³•çš„å¯¹æ¯”\n",
    "\n",
    "* å½“**ç¯å¢ƒçŠ¶æ€è¿‡å¤š**ï¼Œå¦‚ä½•å°†æœ‰é™æ ·æœ¬ä¸­çš„ç­–ç•¥æ¨å¹¿åˆ°æ›´å¤§çš„çŠ¶æ€ç©ºé—´ï¼Œä½œä¸ºæ›´å¤§çŠ¶æ€ç©ºé—´çš„è¿‘ä¼¼è§£ï¼Ÿ\n",
    "    * Method 1: Combined with Supervised Learning:\n",
    "        - When there are too much environment states, it is not feasible to get the action for each state, we can sample limited amount of states, and RL on those to get actions; \n",
    "        - For the rest of states, using Supervised Learning (e.g. Decision Tree, SVM, Logi-Regression...) to predict the action given the state, based on those sampled amount of state-action pair obtained above.\n",
    "        <img src='pic/SupervisedLearning+RL.png'>\n",
    "    * Method 2: çº¿æ€§æ–¹æ³•ç­‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Q-learing+Deep-Learning\n",
    "    * DQN:\n",
    "        - Example: Using CNN to extract feature of environment, output feature is Environment State; Then using RL\n",
    "    * DQNçš„ä¼˜åŠ¿ä¸ç‰¹ç‚¹\n",
    "        <img src='./pic/DQN_Advantages.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Markov Propertyï¼šç°åœ¨å†³å®šæœªæ¥\n",
    "- Definition: The Markov property means that evolution of the Markov process in the future (S_t+1) depends only on the present state (S_t) and does not depend on past history (S_1, S_2, ..., S_t-1). \n",
    "- AKA: **Memoryless Property**\n",
    "* $ P[S_{t+1}|S_t] = P[S_{t+1}|S_1,S_2,S_3,...,S_t]$\n",
    "\n",
    "\n",
    "### 3.2 MarkovçŠ¶æ€è½¬ç§»çŸ©é˜µ - Markov State Transition Matrix\n",
    "- çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼š\n",
    "    - probabilities of transitioning from one state to another in a single time unit\n",
    "    - $ P_{ss^{'}}=P[S_{t+1}=s^{'}|S_t=s]$ -> the probability of state s goes to state s'\n",
    "* çŠ¶æ€è½¬ç§»çŸ©é˜µï¼š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{n,n} =\n",
    " \\begin{pmatrix}\n",
    "  p_{1,1} & p_{1,2} & \\cdots & p_{1,n} \\\\\n",
    "  p_{2,1} & p_{2,2} & \\cdots & p_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  p_{n,1} & p_{n,2} & \\cdots & p_{n,n}\n",
    " \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Markov State Transition Matrix.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* è®¡ç®—ä»¥ä¸‹å›¾ç¤ºçš„çŠ¶æ€è½¬ç§»çŸ©é˜µï¼š\n",
    "<img src='pic/mc_matrix.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> answer:\n",
    "\n",
    "\n",
    "<img src='pic/CalculateMarkovStateTransitionMatrix.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Markov Rewards Process\n",
    "* Definition: Current Reward Rs:\n",
    "    > Rs=E[Rt+1|St=s]\n",
    "* Future Reward: $R_{t+1}, R_{t+2}, ...$\n",
    "* Current Return ğºğ‘¡ is the total discounted reward from time-step t \n",
    "    > $ G_t = R_{t+1} + \\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$\n",
    "    - **$\\gamma$ is called \"discount-rate\" that reflecting the impact of time to when you get the reward.**\n",
    "    - for example: 100 CAD that you get today is different from 100 CAD that you get 10 years later.\n",
    "    - usually: 100 CAD that you get today worths more than 100 CAD that you get 10 years later because of the inflation of currency.\n",
    "    - therefore: $\\gamma$ is [0,1], meaning, the later you get the reward, the less valuable of that reward.\n",
    "* The value function v(s) gives the long-term value of state s:\n",
    "    > The state value function v(s) of an MRP is the **expected return ğºğ‘¡** ***starting from state s***\n",
    "    \n",
    "    > $v(s) = E[G_t|S_t=s]$, where E is Expectation (æœŸæœ›ï¼‰ï¼Œ V is Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we manually define some Reward at each state, the reward is our å¿ƒæƒ…; Then the Overall Value V is to max out our å¿ƒæƒ… overall\n",
    "\n",
    "<img src='pic/Rewards.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 çŠ¶æ€ä»·å€¼state value\n",
    "* ä»çŠ¶æ€S=s å¼€å§‹çš„æ€»ä»·å€¼çš„æœŸæœ›å€¼(as defined above)\n",
    "> $v(s) = E[G_t|S_t=s]$\n",
    "* è®¡ç®—ä¸€ä¸‹ä» class1å¼€å§‹çš„é©¬å°”å¯å¤«è¿‡ç¨‹çš„æ€»ä»·å€¼ï¼Œå½“$ \\gamma = 1/2$ ï¼Ÿ\n",
    "    * C1,C2,C3,PASS,Sleep\n",
    "    * C1,FB,C1,C2,Pub\n",
    "    * C1,FB,FB,FB,FB\n",
    "* åˆ†åˆ«è®¡ç®—ä»¥ä¸Šè·¯å¾„çš„æ€»ä»·å€¼ï¼Œå½“$ \\gamma$ = 0\n",
    "* åˆ†åˆ«è®¡ç®—ä»¥ä¸Šè·¯å¾„çš„æ€»ä»·å€¼ï¼Œå½“$\\gamma$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CurrentçŠ¶æ€ä»·å€¼å¯è¢«åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€æ˜¯å½“å‰çš„å¥–åŠ±ï¼ŒäºŒæ˜¯ä¸‹ä¸€æ­¥çŠ¶æ€çš„æ€»ä»·å€¼çš„è´´ç°å€¼\n",
    "\\begin{equation}\n",
    "v(s) = E[G_t|S_t=s]\\\\\n",
    "=E[R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... |S_t=s]\\\\\n",
    "=E[R_{t+1}|S_t=s]+\\gamma E[R_{t+2}+ \\gamma R_{t+3}+ ...|S_t=s]\\\\\n",
    "=E[R_{t+1}|S_t=s]+\\gamma E[G_{t+1}|S_t=s]\\\\\n",
    "\\end{equation}\n",
    "Please note: for $E[G_{t+1}|S_t=s], S_t=s$ means $S_{t+1}=s_{next}$, so $E[G_{t+1}|S_t=s]$ <=> $E[G_{t+1}|S_{t+1}=s_{next}] = v(s_{next})$\n",
    "\n",
    "\\begin{equation}\n",
    "v(s)=E[R_{t+1}|S_t=s]+\\gamma v(S_{next})|S_t=s\n",
    "\\end{equation}\n",
    "\n",
    "Also based on previous definition: Rs=E[Rt+1|St=s], we have:\n",
    "\n",
    "$v(s)=R_s+\\gamma v(S_{next})|S_t=s$\n",
    "\n",
    "For $v(S_{next})|S_t=s$ part, we can understand it as combination of all possible [next state's value] \\* [weight] (i.e. Probability of going to certain next state):\n",
    "> $\\sum_{s^{'}}P_{ss{'}}v(s^{'})$\n",
    "<img src='./pic/NextState_MarkovRewardProcess.png'>\n",
    "* $v(s) = R_s + \\gamma \\sum_{s^{'}}P_{ss{'}}v(s^{'})$\n",
    "\n",
    "- Using Matrix to express: (This is called **Bellman Equation**ï¼‰\n",
    "> v(s) = R + $\\gamma$P v(s_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bellmanæ–¹ç¨‹\n",
    "* $v(s) = E[G_t|S_t=s]$  -> Bellman Equation for state-value\n",
    "* v(s) = R + $\\gamma$P v(s_next) -> Bellman Equation for state-value\n",
    "* PçŠ¶æ€è½¬ç§»çŸ©é˜µ -> Transition Probability (see 3.2)\n",
    "* ä¸åŠ¨ç‚¹æ±‚è§£:\n",
    "    - If it is in Steady State (equilibrium), all state will not change anymore with time, therefore s=s_next, \n",
    "    \n",
    "    <img src='pic/Bellman_Equation_SteadyState.png'>\n",
    "    - so we can get steady state v:\n",
    "    * v = $(I-\\gamma P)^{-1}R$\n",
    "    \n",
    "    \n",
    "* è¿™ç§ç›´æ¥æ±‚è§£åªå¯¹å°è§„æ¨¡çš„MCæœ‰æ•ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä¾‹1: ä¸€ä¸ªæ£‹ç›˜åªèƒ½ï¼š\n",
    "    * ä¸Šä¸‹å·¦å³èµ°ï¼Œå„å 1/4æ¦‚ç‡\n",
    "    * ä¸èƒ½èµ°å‡ºç•Œï¼Œå¦åˆ™æ£‹å­ä¸åŠ¨ï¼Œå¹¶æ‰£1åˆ†\n",
    "    * ç§»åŠ¨åˆ°A,Bå¤„ï¼Œå„å¾—10ï¼Œ5åˆ†ï¼Œå¹¶åœ¨ä¸‹ä¸€æ­¥ç§»åŠ¨è‡³$A^{'},B^{'}$å¤„\n",
    "    * $\\gamma=0.9$\n",
    "    - Question: What's the steady state's V(s)? (V(s) is a 5x5 matrix)\n",
    "<img src='pic/chess.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Bellman_Equation_Ex1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä¾‹2: 5ä¸ªæ ¼å­ï¼Œæœ€å³çš„rewardsä¸º1ï¼Œå…¶ä½™ä¸º0ã€‚å·¦å³å‡ä¸ºç»ˆæ­¢çŠ¶æ€ã€‚æ ¹æ®Bellmanæ–¹ç¨‹æ±‚v (at steady state)\n",
    "- note: action can be either to left or to right or stay(i.e. at ç»ˆæ­¢çŠ¶æ€ï¼‰ï¼Œ probability is 1/2 for each\n",
    "- assume  $\\gamma=1$\n",
    "<img src='pic/boxes.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Bellman_Equation_Ex2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 ç­–ç•¥ (Policy) $\\pi$\n",
    "* $\\pi (a|s) = P[A_t=a|S_t=s]$ where \"a\" is action (e.g. move left, move right, stay still, ...)\n",
    "* ä¸æ—¶é—´æ— å…³ï¼Œä¸çŠ¶æ€æœ‰å…³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some resource to help understand 3.7 and 3.8:\n",
    "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 çŠ¶æ€ä»·å€¼å‡½æ•°(state-value) v.s åŠ¨ä½œä»·å€¼å‡½æ•° (state-action-value)\n",
    "çŠ¶æ€ä»·å€¼å‡½æ•°: (Bellman Expectation Equation for state-value)\n",
    "* $v_{\\pi}(s) = E_\\pi [G_t|S_t=s]$\n",
    "- we are finding the value of a particular state subjected to some policy(Ï€), this is the difference between Bellman Equation (defined above) and the Bellman Expectation Equation(here).\n",
    "\n",
    "åŠ¨ä½œä»·å€¼å‡½æ•°: (Bellman Expectation Equation for State-Action Value)\n",
    "* åŠ¨ä½œä»·å€¼å‡½æ•°å¤šäº†ä¸€ä¸ªåŠ¨ä½œaï¼Œä»£è¡¨åœ¨çŠ¶æ€sä¸‹åšäº†åŠ¨ä½œaä¹‹åçš„æ€»ä»·å€¼æœŸæœ›\n",
    "* $q_{\\pi}(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$\n",
    "* åŠ¨ä½œä»·å€¼å‡½æ•°ç…§æ ·å¯ä»¥åˆ†è§£ï¼š\n",
    "    * $q_{\\pi}(s,a) = E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰çš„$q_{\\pi}(s,a)$ä¸å½“å‰çš„$v_{\\pi}(s)$çš„å…³ç³»\n",
    "    - Usually in graphic expression: Hollow dot means State; Solid dot means Action\n",
    "    - This backup diagram describes the value of being in a particular state. From the state s there is some probability (pi(a|s)) that we take both the actions. There is a Q-value(State-action value function) for each of the action. We average the Q-values which tells us how good it is to be in a particular state. Basically, it defines VÏ€(s).\n",
    "<img src='pic/v_q.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰çš„$q_{\\pi}(s,a)$ä¸**ä¸‹ä¸€æ­¥**çš„$v_{\\pi}(s^{'})$çš„å…³ç³»\n",
    "    - Tips: When t -> t+1, we need to add $R_s^a$ (see equation below); When t->t (no time step change), we don't need it (see equation above)\n",
    "<img src='pic/q_v.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰çš„$v_{\\pi}(s)$ä¸**ä¸‹ä¸€æ­¥**çš„$v_{\\pi}(s^{'})$çš„å…³ç³»\n",
    "<img src='pic/v_v.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰çš„$q_{\\pi}(s,a)$ä¸**ä¸‹ä¸€æ­¥**çš„$q_{\\pi}(s^{'},a^{'})$çš„å…³ç³»\n",
    "<img src='pic/q_q.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, this is how we can formulate Bellman Expectation Equation for a given MDP to find itâ€™s State-Value Function and State-Action Value Function. \n",
    "\n",
    "But, it does not tell us the best way to behave in an MDP. For that letâ€™s talk about what is meant by Optimal Value and Optimal Policy Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 æœ€ä¼˜æ€»ä»·å€¼(Optimal State-Value Function)ä¸æœ€ä¼˜åŠ¨ä½œä»·å€¼(Optimal State-Action Value Function (Q-Function))\n",
    "* Optimal State-Value Function:\n",
    "    - It is the maximum Value function over all policies.\n",
    "> $v_{*}(s)=max_{\\pi}v_{\\pi}(s)$\n",
    "* Optimal State-Action Value Function (Q-Function):\n",
    "    - It is the maximum action-value function over all policies.\n",
    "> $q_{*}(s,a)=max_{\\pi}q_{\\pi}(s,a)$\n",
    "* å¯¹äºä»»ä½•MDPï¼Œå¿…å®šæœ‰æœ€ä¼˜æ€»ä»·å€¼ä¸æœ€ä¼˜åŠ¨ä½œä»·å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰max-v ä¸ å½“å‰max-qçš„å…³ç³»\n",
    "    - note: only pick one action a, that can max out the q -> q*, therefore all the other actions has 0 probability ($\\pi$)\n",
    "<img src='pic/max_v.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called: **Bellman Optimality Equation for State-value Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰max-q ä¸ ä¸‹ä¸€æ­¥max-vçš„å…³ç³»\n",
    "<img src='pic/max_q.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Action Value Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰max-v ä¸ ä¸‹ä¸€æ­¥max-vçš„å…³ç³»\n",
    "<img src='pic/max_v_v.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Value Function from the Backup Diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* å½“å‰max-q ä¸ ä¸‹ä¸€æ­¥max-qçš„å…³ç³»\n",
    "<img src='pic/max_q_q.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is called **Bellman Optimality Equation for State-Action Value Function from the Backup Diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä¸€èˆ¬æ¥è¯´æœ€ä¼˜çŠ¶æ€ä»·å€¼æˆ–è€…æœ€ä¼˜åŠ¨ä½œä»·å€¼éƒ½æ— æ³•ç›´æ¥æ±‚, reason:\n",
    "    * need to æŒæ¡ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–\n",
    "    * need è¶³å¤Ÿçš„è¿ç®—èƒ½åŠ›\n",
    "    * need to æ»¡è¶³Markovæ€§è´¨\n",
    "* ä¸èƒ½ç›´æ¥æ±‚çš„ï¼Œéœ€è¦ç”¨åˆ°è¿‘ä¼¼æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Flappy birdçš„ç®€å•è§£å†³æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Instead of using DQN (Image->CNN->Feature->RL), we can simplify the problem by manually defining/providing the feature to RL (i.e. Feature -> RL)\n",
    "- Advantage:\n",
    "    - Manual Feature Input will save lots of time of CNN auto detect the feature to use. (Simplified Method: few hundreds training time; DQN Method: Millions training time)\n",
    "- Disadvantage:\n",
    "    - Manual Feature method cannot be applied generally to other problem (e.g. other games), while DQN method can be applied to other games because we don't manually provide the feature to RL, CNN figured out features on its own, we only need to provide pictures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
