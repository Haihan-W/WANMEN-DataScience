{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习1: 基本概念与简单例子\n",
    "\n",
    "* 1.复习监督学习\n",
    "* 2.强化学习系列课程基本概念与方法总览\n",
    "* 3.马尔可夫决策过程\n",
    "    * Markov Decision Processes\n",
    "\n",
    "* 4.Flappy bird的简单解决方法\n",
    "\n",
    "[参考视频David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "[参考书籍Sutton](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)\n",
    "\n",
    "[参考中文知乎](https://www.zhihu.com/people/flood-sung/activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.复习监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Supervised.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.强化学习系列课程基本概念与方法总览\n",
    "    * 2.1强化学习基本概念\n",
    "        * 特点：\n",
    "        1. 没有预先设定的label\n",
    "        2. 反馈往往是滞后的\n",
    "        3. 目标：累计奖励最大化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='RL_1.png',width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 个体agent\n",
    "    * 动作\n",
    "    * 总策略\n",
    "    * 例子：毕业后的选择，人生策略\n",
    "* 奖励\n",
    "    * 例子：毕业后的选择\n",
    "* 总价值\n",
    "    * $v_\\pi(S) = E_\\pi[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}...|S_t=s]$\n",
    "    * 例子：毕业\n",
    "* 总策略\n",
    "* 环境模型(model)\n",
    "    * 预判下一步状态以及概率\n",
    "        * $P_{ss^{'}}^{a}=P[S_{t+1}=s^{'}|S_t=s,A_t=a]$\n",
    "    * 预判下一步的收益\n",
    "* 环境状态\n",
    "* 强化学习：在不断地‘了解环境’与‘最大化已知环境下的总收益’\n",
    "    * learning vs planing \n",
    "    * exploring探索未知 vs exploitation利用已知\n",
    "    * 选餐馆\n",
    "    * 选择钻井位置\n",
    "    * 选工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Agent_Env.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2 策略与总价值 例子，走迷宫（选自David Silver讲义）\n",
    "<img src='policy.png',width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='value.png',width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 强化学习系列方法总览\n",
    "* Flappy bird的简单解决方法\n",
    "    * 如何衡量总价值\n",
    "    * 如何选择动作，选择总策略\n",
    "* 总价值不易计算时，但环境状态有显式的分布时\n",
    "    * 如何使用迭代法计算总价值\n",
    "    * 如何使用迭代法反复改进总策略\n",
    "    * 策略迭代法的收敛\n",
    "* 总价值不易计算时，环境状态没有显式的分布时，从连续的样本和经验中学习\n",
    "    * 蒙特卡洛方法\n",
    "    * 计算总价值\n",
    "    * 更新总策略\n",
    "* 总价值不易计算时，环境状态没有显式的分布时，从每一次与环境状态的交互中学习\n",
    "    * Temporal Differences\n",
    "    * Temporal Differences与蒙特卡罗方法的对比\n",
    "    * SARSA\n",
    "    * Q-learning\n",
    "* 当环境状态过多，如何将有限样本中的策略推广到更大的状态空间，作为更大状态空间的近似解？\n",
    "    * 结合监督学习\n",
    "    * 线性方法等\n",
    "* Q-learing+Deep-Learning\n",
    "    * DQN\n",
    "    * DQN的优势与特点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.马尔可夫决策过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Markov性质：现在决定未来\n",
    "* $ P[S_{t+1}|S_t] = P[S_{t+1}|S_1,S_2,S_3,...,S_t]$\n",
    "\n",
    "### 3.2 Markov状态转移矩阵\n",
    "* 状态转移概率：$ P_{ss^{'}}=P[S_{t+1}=s^{'}|S_t=s]$\n",
    "* 状态转移矩阵：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{m,n} =\n",
    " \\begin{pmatrix}\n",
    "  p_{1,1} & p_{1,2} & \\cdots & p_{1,n} \\\\\n",
    "  p_{2,1} & p_{2,2} & \\cdots & p_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  p_{m,1} & p_{m,2} & \\cdots & p_{m,n}\n",
    " \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 计算以下图示的状态转移矩阵：\n",
    "<img src='mc_matrix.png',width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Markov Rewards Process\n",
    "* $R_{t+1}, R_{t+2}, ...$\n",
    "* Return $ G_t = R_{t+1} + \\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$\n",
    "* 计算未来收益的贴现值\n",
    "* 为什么？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Rewards.png',width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 状态价值state value\n",
    "* 从状态S=s 开始的总价值的期望值\n",
    "* $v(s) = E[G_t|S_t=s]$\n",
    "* 计算一下从 class1开始的马尔可夫过程的总价值，当$ \\gamma = 1/2$ ？\n",
    "    * C1,C2,C3,PASS,Sleep\n",
    "    * C1,FB,C1,C2,Pub\n",
    "    * C1,FB,FB,FB,FB\n",
    "* 分别计算以上路径的总价值，当$ \\gamma$ = 0\n",
    "* 分别计算以上路径的总价值，当$\\gamma$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 状态价值可被分为两个部分，一是当前的奖励，二是下一步状态的总价值的贴现值\n",
    "\\begin{equation}\n",
    "v(s) = E[G_t|S_t=s]\\\\\n",
    "=E[R_{t+1}+\\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... |S_t=s]\\\\\n",
    "=E[R_{t+1}+\\gamma (R_{t+1}+ \\gamma R_{t+2}+ ...)|S_t=s]\\\\\n",
    "=E[R_{t+1}+\\gamma G_{t+1}|S_t=s]\\\\\n",
    "v(s)=E[R_{t+1}+\\gamma v(S_{t+1})|S_t=s]\n",
    "\\end{equation}\n",
    "* $v(s) = R_s + \\gamma \\sum_{s^{'}}P_{ss{'}}v(s^{'})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bellman方程\n",
    "* v(s) = R + $\\gamma$P v\n",
    "* P状态转移矩阵\n",
    "* 不动点求解\n",
    "    * v = $(I-\\gamma P)^{-1}R$\n",
    "* 这种直接求解只对小规模的MC有效\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例1: 一个棋盘只能：\n",
    "    * 上下左右走，各占1/4概率\n",
    "    * 不能走出界，否则棋子不动，并扣1分\n",
    "    * 移动到A,B处，各得10，5分，并在下一步移动至$A^{'},B^{'}$处\n",
    "    * $\\gamma=0.9$\n",
    "<img src='chess.png',width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例2: 5个格子，最右的rewards为1，其余为0。左右均为终止状态。根据Bellman方程求v\n",
    "<img src='boxes.png',width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 策略$\\pi$\n",
    "* $\\pi (a|s) = P[A_t=a|S_t=s]$\n",
    "* 与时间无关，至于状态有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 状态价值函数 v.s 动作价值函数\n",
    "* $v_{\\pi}(s) = E_\\pi [G_t|S_t=s]$\n",
    "* 动作价值函数多了一个动作a，代表在状态s下做了动作a之后的总价值期望\n",
    "* $q_{\\pi}(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$\n",
    "* 动作价值函数照样可以分解：\n",
    "    * $q_{\\pi}(s,a) = E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与当前的$v_{\\pi}(s)$的关系\n",
    "<img src='v_q.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与**下一步**的$v_{\\pi}(s^{'})$的关系\n",
    "<img src='q_v.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$v_{\\pi}(s)$与**下一步**的$v_{\\pi}(s^{'})$的关系\n",
    "<img src='v_v.png',width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前的$q_{\\pi}(s,a)$与**下一步**的$q_{\\pi}(s^{'},a^{'})$的关系\n",
    "<img src='q_q.png',width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 最优总价值与最优动作价值\n",
    "* $v_{*}(s)=max_{\\pi}v_{\\pi}(s)$\n",
    "* $q_{*}(s,a)=max_{\\pi}q_{\\pi}(s,a)$\n",
    "* 对于任何MDP，必定有最优总价值与最优动作价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-v 与 当前max-q的关系\n",
    "<img src='max_v.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-q 与 下一步max-v的关系\n",
    "<img src='max_q.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-v 与 下一步max-v的关系\n",
    "<img src='max_v_v.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当前max-q 与 下一步max-q的关系\n",
    "<img src='max_q_q.png',width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一般来说最优状态价值或者最优动作价值都无法直接求\n",
    "    * 掌握环境的动态变化\n",
    "    * 足够的运算能力\n",
    "    * 满足Markov性质\n",
    "* 不能直接求的，需要用到近似方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Flappy bird的简单解决方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
