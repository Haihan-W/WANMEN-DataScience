{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/study_workflow.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.Temperal-Difference Learning\n",
    "    * 时间差分法公式\n",
    "    * 与蒙特卡洛的对比\n",
    "* 2.Model Free Control\n",
    "    * $\\epsilon$-Greedy Policy\n",
    "    * on-policy v.s off-policy\n",
    "    * sarsa and Q-learning\n",
    "* 3.函数近似方法(对V,q的近似)\n",
    "* 4.DQN\n",
    "* 5.Flappy Bird using DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.temperal-difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 公式\n",
    "$ V(S_t)\\leftarrow V(S_t) + \\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_{t}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 与蒙特卡洛法的对比\n",
    "$ V(S_t)\\leftarrow V(S_t) + \\alpha(G_t-V(S_{t}))$, where $G_t = R_{t+1}+\\gamma R_{t+2} + ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MC的特点\n",
    "    * 不到黄河心不死，不见棺材不流泪\n",
    "    * 必须在有终止条件的环境中使用 (e.g. flappy bird hit the pillar - this is a terminal condition)\n",
    "* TD的特点\n",
    "    * 每走一步都可以从中更新价值函数\n",
    "    * 可以在没有终止条件环境中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/drive.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Example: from office drive to home, MC vs. TD predicted total travel time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/drive_1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 是否是无偏估计？(unbias)\n",
    "    * 蒙特卡洛方法的 $G_t = R_{t+1}+\\gamma R_{t+2} + ... $是无偏估计\n",
    "    * TD方法的真实估计量 $R_{t+1}+\\gamma V_{\\pi}(S_{t+1}) $是无偏估计\n",
    "    * TD方法的估计量 $R_{t+1}+\\gamma V(S_{t+1}) $是有偏估计\n",
    "        * 因为$V(S_{t+1})$只是$V_{\\pi}(S_{t+1})$的近似值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 收敛性质\n",
    "    * 蒙特卡洛方法收敛速度一般不快\n",
    "    * TD收敛速度一般比较快\n",
    "    * 蒙特卡洛方法对初始值不敏感\n",
    "    * TD方法较易受初始值影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/td_mc_question.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/td_mc_result.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 收敛性质\n",
    "    * MC, TD有可能收敛到不同到值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MC v.s TD\n",
    "    * A,B例子：A,B两个状态，实验了8次\n",
    "    * A,0,B,0\n",
    "    * B,1\n",
    "    * B,1\n",
    "    * B,1\n",
    "    * B,1\n",
    "    * B,1\n",
    "    * B,1\n",
    "    * B,0\n",
    "    * 求V(A),V(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/ab.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MC收敛到与样本分布最近的解\n",
    "* TD收敛到马尔可夫最大似然的解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Reinforcement.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model-free control\n",
    "* greedy policy\n",
    "* $\\epsilon$-soft policy\n",
    "* $\\epsilon$-greedy exploring policy\n",
    "* SARSA\n",
    "* Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\epsilon$-greedy exploring policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/epsilon.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Epsilon meaning: (e.g. Epsilon = 0.1 in flappy bird project)\n",
    "    \n",
    "    - 10% chance: bird will random fly upwards (10%*50%) or downwards (10%*50%) --> EXPLORING\n",
    "    - 90% chance: bird will choose fly upwards or downwards based on the maximum q(s,a)'s corresponding action. --> Greedy EXPLOITATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exploring (random trials) vs. Exploitation (i.e. greedy)\n",
    "- always want a balance between the two, within the limited amount of computation resource and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* prove $\\epsilon-greedy$比$\\epsilon-soft$好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/prove_epsilon.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on-policy v.s off-policy\n",
    "* behavior policy与target policy的分离\n",
    "* on-policy learning:\n",
    "    * 从用$\\pi$策略取的样本中估计V，按$\\pi$策略更新\n",
    "* off-policy learning:\n",
    "    * 从用$\\pi$策略取的样本中估计V，按$\\pi'$策略更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA is on-policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/sarsa.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/sarsa_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 收敛条件\n",
    "    * $\\sum \\alpha_t = \\infty$\n",
    "    * $\\sum \\alpha_t^2 < \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-learning is off-policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/q-learning.png'>\n",
    "<img src='pic/q-learning_1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- note: Full Backup (DP) is iterative method\n",
    "<img src='pic/summary.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.函数近似方法 (Function Approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "for flappy bird project (chapter 49), we only estimate state value of the bird every 10 pixels (we divided the whole grid to 10x10 pixel grid), therefore the state value is discrete function.\n",
    "\n",
    "<img src='pic/flappybird_Discrete.png'>\n",
    "\n",
    "For inbetween each 10 pixels, we can use Func Approximation method to estimate, e.g. using Supervised Learning, to get a continuous value of states\n",
    "\n",
    "Supervised learning compared estimates with v_pi or q_pi, and improve the fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/fp.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/fp_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for graph above, the bottom is input, the top outlet is output from supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluation Method: Loss Function - 误差平方和最小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/fp_2.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/fp_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models：\n",
    "    * 线性组合\n",
    "    * 树形模型\n",
    "    * 神经网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.DQN\n",
    "* 借助CNN预测价值状态V(s)或者动作状态函数q(s,a)\n",
    "* experience replay\n",
    "* 锁定参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, in flappy bird project, instead of providing delta_x, delta_y as the feature, we provide picure as input and use CNN to study the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 借助CNN预测价值状态V(s)或者动作状态函数q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/cnn.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* experience replay从过去的经验中选择历史来更新参数\n",
    "    * $(s_t,a_t,r_{t+1},s_{t+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 锁定参数\n",
    "\n",
    "<img src='pic/fix.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. flappy bird: (see flappybirdcode/qlearn.py)\n",
    "    * f0.区分训练和run模式\n",
    "    * f1.搭建卷积神经网 CNN\n",
    "    * f2.定义强化学习训练过程\n",
    "        * f2.1 游戏控制与强化学习参数更新\n",
    "        * f2.2 定义贪心学习\n",
    "        * f2.3 积累experience replay\n",
    "        * f2.4 minibatch 训练\n",
    "        * f2.5 更新卷积神经网参数，让loss function最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37tf_gpu]",
   "language": "python",
   "name": "conda-env-py37tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
