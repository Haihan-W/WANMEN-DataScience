{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在这个小例子里面 我们会学习到：\n",
    "\n",
    "1）用name scope 去划分 graph\n",
    "2）写summary 为了可视化\n",
    "3）用sess 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build 5 sections (name_scopes) at highest level of the graph, under each section, we may insert sub-level section (name_scopes) as needed\n",
    "1. Variable: to store two variables (non for training): a. count of training times; b. output\n",
    "2. Transformation: operations of input tensors (e.g. In the later Chapters, this can be Neural Network)\n",
    "3. Update: update output/count of training times after each training loop\n",
    "4. summary: create tensor board summary, used for tensor board scalars\n",
    "5. Global_operations: Initialze, Merge Summary for each training loop to one summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this name scope contains variables not for training\n",
    "with tf.name_scope(\"variables\"):\n",
    "    #variable indicating how many times the graph (training loop) run\n",
    "    global_step=tf.Variable(0,dtype=tf.int32,trainable=False,name=\"global_step\")\n",
    "    \n",
    "    #Variable of sum of outputs\n",
    "    total_outputs=tf.Variable(0.0,dtype=tf.float32,trainable=False,name=\"total_output\")\n",
    "\n",
    "#this name scope contains variables for transformation    \n",
    "with tf.name_scope(\"transformation\"):\n",
    "    \n",
    "    #sub-level name scope1: define input\n",
    "    with tf.name_scope(\"input\"):\n",
    "        a=tf.placeholder(tf.float32, shape=(None), name='input_placeholder_a') #shape=(None) means any shape is fine\n",
    "        \n",
    "    #sub-level name scope2: define middle operations variables\n",
    "    with tf.name_scope(\"middle\"):\n",
    "        b=tf.reduce_prod(a,name='production_b')\n",
    "        c=tf.reduce_sum(a,name='sum_c')\n",
    "        \n",
    "    #sub-level name scope3: define output variable for each transformation\n",
    "    with tf.name_scope(\"output\"):\n",
    "        output=tf.add(b,c, name='output')\n",
    "    \n",
    "\n",
    "#this name scope update variables:\n",
    "with tf.name_scope('update'):\n",
    "    #add output to total_output\n",
    "    total_outputs_copy=total_outputs.assign_add (output) #add each training loop (transformation)'s output to total_outputs\n",
    "    global_step_copy=global_step.assign_add(1) #add 1 after each loop to global_step\n",
    "    \n",
    "    #please note: assign_add() here will directly change \"total_outputs\" and \"global steps\" (referred as xxx and xxx in below), \n",
    "    # and return a copy of updated xxx and xxx\n",
    "    # I used a new variable (total_outputs_copy and global_step_copy) to store the copy of updated value because later on I may change the data type for calculate purpose (e.g. calculate average below)\n",
    "    # therefore I don't want to directly change on the original variable xxx and xxx, but to change datatype on that copy\n",
    "    \n",
    "    ##This treatment above is the best practice in industry##\n",
    "    \n",
    "#This name scope create tensor board summaries\n",
    "with tf.name_scope('summaries'):\n",
    "    average=tf.div(total_outputs_copy, tf.cast(global_step_copy,tf.float32), name='average') \n",
    "    #tf.cast() here: convert dtype of global_step_copy to float32, \n",
    "    # reason: tf.div() can only accept input arguments to be the same dtype, but total_outputs_copy was float32 while global_step_copy was int32\n",
    "    \n",
    "    \n",
    "    tf.summary.scalar('output_summary', output)\n",
    "    tf.summary.scalar('total_summary', total_outputs_copy)\n",
    "    tf.summary.scalar('average_summary', average)\n",
    "    \n",
    "#This name scope initialize variable (globally) and merge summary under each training loop(transformation)\n",
    "with tf.name_scope('global_operations'):\n",
    "    #Initialize\n",
    "    init=tf.global_variables_initializer() #global_variables_initializer can initialize all variable at once, comparing to variables_initializer which can only initialize the specfic variable \n",
    "    #Merge all summaries generated under each training loop to one summary\n",
    "    merged_summaries=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start a session\n",
    "sess=tf.Session()\n",
    "#Open a summary writer\n",
    "summary_writer=tf.summary.FileWriter('./TFExample1',sess.graph)\n",
    "#Initialize: run the \"init\" variable which is previously defined\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph(input_tensor):\n",
    "    feed_dict={a: input_tensor}\n",
    "    \n",
    "    #run the following three variables given feed_dict\n",
    "    outpt,step,summary = sess.run([output,global_step_copy,merged_summaries], feed_dict=feed_dict)\n",
    "    \n",
    "    print(\"step: \", step)\n",
    "    print(\"output: \", outpt)\n",
    "    \n",
    "    #add something into summary_writer\n",
    "    summary_writer.add_summary(summary,step)\n",
    "    summary_writer.add_graph(sess.graph) #note: because in code above we opened a summary writer \"tf.summary.FileWriter(xxx, sess.graph)\"\n",
    "                                        # before run session, so we need to update the graph after run session using add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1\n",
      "output:  26.0\n",
      "step:  2\n",
      "output:  8.0\n"
     ]
    }
   ],
   "source": [
    "run_graph([2,8]) #first training loop (transformation): output=b+c=(2x8)+(2+8)=26\n",
    "run_graph([3,1,1]) #second training loop (transformation): output=b+c=(3x1x1)+(3+1+1)=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this method to make sure that all pending events have been written to disk\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close\n",
    "summary_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see Graph and Summary on Tensor Board:\n",
    "- anaconda prompt: navigate to Example1 folder, run\n",
    "> tensorboard --logdir=./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35tf]",
   "language": "python",
   "name": "conda-env-py35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
