{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: Using Logistic Regression in TF to recognize the following handwritten number from MINST database\n",
    "\n",
    "<img src='./pic/MnistExamples.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data # import MNIST database\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 手写数字集合有55000 training images and 10000 testing images, 5000 validation images, 均是28*28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01 \n",
    "batch_size=128 #一次load 128 pictures\n",
    "n_epochs=30 # run 30 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('../MNIST-data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: setup two place holder x and y\n",
    "- shape for x: [batch_size, 784] shape for y: [batch_size, 10]\n",
    "- 784=28x28 (each image size is 28x28, 我们把每张图都拉平从2维到一维\n",
    "- 10 means there are 10 labels (handwrite letters: 0~9) in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [batch_size, 784], name=\"X_placeholder\")\n",
    "Y=tf.placeholder(tf.int32, [batch_size, 10], name=\"Y_placeholder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use Variable to Express w and b (Y=Xw+b regression)\n",
    "- W has shape of [784, 10] -> Xw will be [batch_size, 784]x[784, 10] = [batch_size, 10] shape\n",
    "- b has shape of [1,10] -> I don't understand why b shape is not [batch_size,10]??? It was not explained well in tutorial\n",
    "    - For now, I used [batch_size,10] as shape of b, looks like it performed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal(shape=[784,10], stddev=0.01), name='weights') #mean is 0.0 by default\n",
    "b=tf.Variable(tf.zeros([batch_size,10]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Define 线性层 (i.e. relationship between predicted Y and X,w,b-> Ypred=Xw+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(128, 10) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit=tf.matmul(X,w)+b\n",
    "logit #please note the shape of the logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Define Cross-entropy Loss function\n",
    "- 1. using logit -> normalize logit with log function (i.e. Y between 0 and 1), then calculate cross_entropy, then normalize cross_entropy (loss) using softmax (normalize loss so that Sum(loss)=1)\n",
    "- 2. Calculate mean of loss across all examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=Y, name='loss') #step1 above\n",
    "loss = tf.reduce_mean(cross_entropy) #step2 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Define the training method to train Loss Function to minimize loss\n",
    "- here we are using GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 0 is 1.290301343638858\n",
      "Average loss for epoch 1 is 0.7351923608557606\n",
      "Average loss for epoch 2 is 0.6028370971863086\n",
      "Average loss for epoch 3 is 0.5398999777012494\n",
      "Average loss for epoch 4 is 0.5009538739016561\n",
      "Average loss for epoch 5 is 0.4734705094690923\n",
      "Average loss for epoch 6 is 0.45539031717882844\n",
      "Average loss for epoch 7 is 0.4390068252881368\n",
      "Average loss for epoch 8 is 0.4262276328665949\n",
      "Average loss for epoch 9 is 0.4171778878548762\n",
      "Average loss for epoch 10 is 0.40778209147475536\n",
      "Average loss for epoch 11 is 0.401035820826506\n",
      "Average loss for epoch 12 is 0.39446703617250445\n",
      "Average loss for epoch 13 is 0.38763466435712535\n",
      "Average loss for epoch 14 is 0.3841316726032671\n",
      "Average loss for epoch 15 is 0.3795027020441624\n",
      "Average loss for epoch 16 is 0.37313466503486764\n",
      "Average loss for epoch 17 is 0.3716307927470107\n",
      "Average loss for epoch 18 is 0.3671290382638678\n",
      "Average loss for epoch 19 is 0.36432908173207634\n",
      "Average loss for epoch 20 is 0.3606906653770478\n",
      "Average loss for epoch 21 is 0.35971156457365255\n",
      "Average loss for epoch 22 is 0.3557203216847284\n",
      "Average loss for epoch 23 is 0.3521245523835673\n",
      "Average loss for epoch 24 is 0.3524564203126725\n",
      "Average loss for epoch 25 is 0.3490358232618212\n",
      "Average loss for epoch 26 is 0.3487296237464829\n",
      "Average loss for epoch 27 is 0.3451994554701941\n",
      "Average loss for epoch 28 is 0.34221942497022223\n",
      "Average loss for epoch 29 is 0.34200560570735755\n",
      "Total running time: 11.620187520980835 seconds\n",
      "Accuracy on testing sets is 0.9104\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #visualize using tensorboard\n",
    "    writer = tf.summary.FileWriter('./TFExample2_Logistic_reg', sess.graph)\n",
    "    start_time=time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    ######Training######\n",
    "    n_batches=int(mnist.train.num_examples/batch_size) #calculate how many batches in total in MNIST training data set\n",
    "    \n",
    "    #train model n_epochs times\n",
    "    for i in range(n_epochs): \n",
    "        total_loss=0 #initialize from 0\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            \n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size) #fetch data for X and Y for each training batch\n",
    "            \n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch}) #loss_batch is the loss calculated from each training batch\n",
    "            \n",
    "            total_loss+=loss_batch\n",
    "            \n",
    "        avg_loss_per_epoch=total_loss/n_batches\n",
    "        \n",
    "        print ('Average loss for epoch {0} is {1}'.format(i, avg_loss_per_epoch))\n",
    "        \n",
    "    print ('Total running time: {0} seconds'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    ######Testing Model Accuracy######\n",
    "    \n",
    "    n_batches=int(mnist.test.num_examples/batch_size) #calculate how many batches in total in MNIST testing data set\n",
    "    \n",
    "    #define some variables (tensors) for testing\n",
    "    Y_predicted = logit\n",
    "    \n",
    "    correct_preds_boolean = tf.equal(tf.argmax(Y_predicted,axis=1), tf.argmax(Y, axis=1)) #tf.equal(A,B) -> if(A=B) then 1 else 0\n",
    "        #reason to use argmax:\n",
    "            #output label 0~9 is using onehot encoding, i.e. [0,0,0,0,0,0,0,0,0,1] -> means output label is \"9\" (index=9)\n",
    "                                                            #[1,0,0,0,0,0,0,0,0,0] -> means output label is \"0\" (index=0)\n",
    "            #since argmax can return the index/position of the max value along the axis, it can help us find the corresponding output label(0~9)\n",
    "            #note: usually only 1 prediction is made for each label, so there should only be one \"1\" in each row of onehot encoding.\n",
    "                \n",
    "                \n",
    "    count_of_correct_predictions=tf.reduce_sum(tf.cast(correct_preds_boolean, tf.float32))\n",
    "    \n",
    "    total_correct_preds=0 #initialize from 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size) #fetch data for X and Y for each testing batch\n",
    "        count_of_correct_predictions_per_batch = sess.run(count_of_correct_predictions, feed_dict={X: X_batch, Y: Y_batch})\n",
    "        total_correct_preds+=count_of_correct_predictions_per_batch\n",
    "        \n",
    "    print ('Accuracy on testing sets is {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    writer.close()\n",
    "    sess.close()\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see Graph and Summary on Tensor Board:\n",
    "- anaconda prompt: navigate to Example2 folder, run\n",
    "> tensorboard --logdir=./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35tf]",
   "language": "python",
   "name": "conda-env-py35tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
