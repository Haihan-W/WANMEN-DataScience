{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习2: 迭代法\n",
    "* 0.强化学习中如何用到迭代法？\n",
    "* 1.迭代法\n",
    "* 2.策略迭代与价值迭代\n",
    "* 3.复习动态规划 (not that related, ignored and deleted in this lecture notes, but mentioned in Lecture Video 48.13-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/RL_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Flappy bird的简单解决方法\n",
    "    * 如何衡量总价值\n",
    "    * 如何选择动作，选择总策略\n",
    "\n",
    "---------------THIS CHAPTER FOCUSED ON --------------------\n",
    "* **总价值不易计算时，但环境状态有显式的分布时**\n",
    "    * 如何使用迭代法计算总价值\n",
    "    * 如何使用迭代法反复改进总策略\n",
    "    * 策略迭代法的收敛\n",
    "\n",
    "----------------END-------------------------------------------    \n",
    "\n",
    "* 总价值不易计算时，环境状态没有显式的分布时，从连续的样本和经验中学习\n",
    "    * 蒙特卡洛方法\n",
    "    * 计算总价值\n",
    "    * 更新总策略\n",
    "* 总价值不易计算时，环境状态没有显式的分布时，从每一次与环境状态的交互中学习\n",
    "    * Temporal Differences\n",
    "    * Temporal Differences与蒙特卡罗方法的对比\n",
    "    * SARSA\n",
    "    * Q-learning\n",
    "* 当环境状态过多，如何将有限样本中的策略推广到更大的状态空间，作为更大状态空间的近似解？\n",
    "    * 结合监督学习, function approximation\n",
    "    * 线性方法等\n",
    "* Q-learing+Deep-Learning\n",
    "    * DQN\n",
    "    * DQN的优势与特点\n",
    "\n",
    "\n",
    "[参考视频](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "[参考书籍](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)\n",
    "\n",
    "[参考中文知乎](https://www.zhihu.com/people/flood-sung/activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.迭代法用在哪里？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 在计算策略总价值时\n",
    "* 当完全知道环境的动态变化，但计算不方便时\n",
    "<img src='pic/v_pi.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 采用近似算法计算在策略$\\pi$下的总价值\n",
    "<img src='pic/v_k.png'>\n",
    "* 最终计算而得的不动点，就应该是$v_\\pi(s)$\n",
    "* 这个步骤，叫做 **iterative policy evaluation 迭代法求策略总价值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 在更新策略，求最优策略时\n",
    "* 迭代法求最优策略\n",
    "* 求对应策略总价值 -> 更新策略 -> 求对应策略总价值 -> 更新策略\n",
    "* GPI (General Policy Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 迭代法\n",
    "* 1.数值计算的基本方法：迭代法解线性方程组\n",
    "    * Gauss消元法的复杂度是n3，迭代法复杂度是n2\n",
    "* 2.Jacob方法\n",
    "* 3.Gauss-Seidel方法\n",
    "* 4.收敛性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Jacob方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "9x_1 + x_2 + x_3 = b_1\\\\\n",
    "2x_1 + 10x_2 + 3x_3 = b_2\\\\\n",
    "3x_1+4x_2+11x_3 = b_3\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 变形\n",
    "\\begin{equation}\n",
    "x_1 = 1/9 [b_1 - x_2-x_3]\\\\\n",
    "x_2 = 1/10 [b_2 -2x_1-3x_3]\\\\\n",
    "x_3 = 1/11[b_3-3x_1-4x_2]\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 初始化 $ x^(0) = [x_1^{(0)},x_2^{(0)},x_3^{(0)}]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 反复迭代\n",
    "* 变形\n",
    "\\begin{equation}\n",
    "x_1^{(k+1)} = 1/9 [b_1 - x_2^{(k)}-x_3^{(k)}]\\\\\n",
    "x_2^{(k+1)} = 1/10 [b_2 -2x_1^{(k)}-3x_3^{(k)}]\\\\\n",
    "x_3^{(k+1)} = 1/11[b_3-3x_1^{(k)}-4x_2^{(k)}]\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例如，当b=$[10,19,0]^T$时，精确解应为： $x=[1,2,-1]^T$\n",
    "* 利用Jacob方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacob_iter(x,b):\n",
    "    x_new = [0,0,0]\n",
    "    x_new[0]=1/9*(b[0]-x[1]-x[2])\n",
    "    x_new[1]=1/10*(b[1]-2*x[0]-3*x[2])\n",
    "    x_new[2]=1/11*(b[2]-3*x[0]-4*x[1])\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.111111,1.900000,0.000000\n",
      "0.900000,1.677778,-0.993939\n",
      "1.035129,2.018182,-0.855556\n",
      "0.981930,1.949641,-1.016192\n",
      "1.007395,2.008472,-0.976760\n",
      "0.996476,1.991549,-1.005097\n",
      "1.001505,2.002234,-0.995966\n",
      "0.999304,1.998489,-1.001223\n",
      "1.000304,2.000506,-0.999260\n",
      "0.999862,1.999717,-1.000267\n",
      "1.000061,2.000108,-0.999859\n",
      "0.999972,1.999946,-1.000056\n",
      "1.000012,2.000022,-0.999973\n",
      "0.999994,1.999989,-1.000011\n",
      "1.000002,2.000005,-0.999995\n",
      "0.999999,1.999998,-1.000002\n",
      "1.000000,2.000001,-0.999999\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n"
     ]
    }
   ],
   "source": [
    "x=[0,0,0]\n",
    "b=[10,19,0]\n",
    "for i in range(20):\n",
    "    x=jacob_iter(x,b)\n",
    "    print(\"%f,%f,%f\"%(x[0],x[1],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gauss-Seidel迭代法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "9x_1 + x_2 + x_3 = b_1\\\\\n",
    "2x_1 + 10x_2 + 3x_3 = b_2\\\\\n",
    "3x_1+4x_2+11x_3 = b_3\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 变形\n",
    "\\begin{equation}\n",
    "x_1 = 1/9 [b_1 - x_2-x_3]\\\\\n",
    "x_2 = 1/10 [b_2 -2x_1-3x_3]\\\\\n",
    "x_3 = 1/11[b_3-3x_1-4x_2]\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 反复迭代\n",
    "* 变形\n",
    "\\begin{equation}\n",
    "x_1^{(k+1)} = 1/9 [b_1 - x_2^{(k)}-x_3^{(k)}]\\\\\n",
    "x_2^{(k+1)} = 1/10 [b_2 -2x_1^{(k+1)}-3x_3^{(k)}]\\\\\n",
    "x_3^{(k+1)} = 1/11[b_3-3x_1^{(k+1)}-4x_2^{(k+1)}]\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gs_iter(x,b):\n",
    "    x_new = [0,0,0]\n",
    "    x_new[0]=1/9*(b[0]-x[1]-x[2])\n",
    "    x_new[1]=1/10*(b[1]-2*x_new[0]-3*x[2])\n",
    "    x_new[2]=1/11*(b[2]-3*x_new[0]-4*x_new[1])\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.111111,1.677778,-0.913131\n",
      "1.026150,1.968709,-0.995753\n",
      "1.003005,1.998125,-1.000138\n",
      "1.000224,1.999997,-1.000060\n",
      "1.000007,2.000017,-1.000008\n",
      "0.999999,2.000003,-1.000001\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n",
      "1.000000,2.000000,-1.000000\n"
     ]
    }
   ],
   "source": [
    "x=[0,0,0]\n",
    "b=[10,19,0]\n",
    "for i in range(20):\n",
    "    x=gs_iter(x,b)\n",
    "    print(\"%f,%f,%f\"%(x[0],x[1],x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 收敛性\n",
    "* 对方程组Ax=b\n",
    "* can be converted to: $Nx^{(k+1)}=b+Px^{k}$\n",
    "* Jacob方法：\n",
    "    * N为对角线矩阵，P为N-A\n",
    "* G-S方法\n",
    "    * N为下三角矩阵，P为N-A\n",
    "\n",
    "Tell if converge or not:\n",
    "* $N(x-x^{(k+1)})=P(x-x^{(k)})$\n",
    "* $e^{(k+1)}=N^{-1}Pe^{(k)}$\n",
    "\n",
    "* 收敛(converge) condition:\n",
    "$N^{-1}P$ matrix 主对角线elements all <1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 迭代法求策略估值与迭代法更新最佳策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.迭代法求策略估值 （iterative process to get State Value under current policy) - Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/v_pi.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/v_k.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p is the transition probability (see Chapter 47), here p(s',r|s,a): means the probability of transit from s -> s', if taking action a.\n",
    "\n",
    "k, k+1 means: current iterative, next iterative.\n",
    "- Iterative Method:\n",
    "    - here we cannot directly get the steady state v_pi, because the steady state v_pi (s) means = v_pi(s'), and we need to solve bellman equation by matrix inversion (see Chapter 47), the complexity of solving that equation is very high when the matrix dimension is very large\n",
    "    - Therefore we use an iterative method to get the numerical solution that is approximately equal to the accurate solution of directly solving the equation.\n",
    "    - In terms of vk(s'), because it is steady-state, therefore vk(s')=vk(s), we can use vk(s) above formula to get the numerical estimation of vk+1(s)\n",
    "    - see below for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/policy_ev_alg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例子\n",
    "<img src='pic/policy_ev.png'>\n",
    "\n",
    "note: Current Policy: pi(a|s): probability of taking each of the 4 actions (see graph above) under whatever state (s) is 1/4\n",
    "\n",
    "note: p(s',r|s,a) is 1, for whatever action taken. \n",
    "> For example: current state s is at (1,1), if take action = move right, the next state s' must be at (1,2), therefore the probability is 1.\n",
    "> Which means each action can only result in one and only one state at t+1. it cannot result in multiple possible states at t+1.\n",
    "\n",
    "note: pre-define: v(s) at position (1,1) = 0; v(s) at position (4,4) = 0\n",
    "\n",
    "note: pre-define: $\\gamma$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- k=0: initialization: all state's value v(s) is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/policy_ev_table.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- k=n, where |v_n - v_n-1| < $\\theta$ (e.g. 0.0001) , then we stop iteration, and get the the steady-state V(s) under current policy Pi  -> V_pi(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note: The example above only update the V(s) under current Policy (which is a random policy: probability of 1/4 moving each direction), but we want to find the optimal policy as well.\n",
    "In this case, we can not only iterative update V(s), but also update Policy (Pi) under EACH iteration at the same time. All the way until both Policy (Pi) and V(s) converged (i.e. difference between last iteration and second last iteration < threshold)**\n",
    "\n",
    "**See section below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.迭代法更新最佳策略 (Iterative process get Optimal Policy and its State Value) - Iterative Policy Evaluation and Greedy Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/policy_iter.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "    - In this Example, how to update Policy (Pi)?\n",
    "        - move to the directions with the maximum State Value\n",
    "        - For example: (see graph below) when k=2, under position (2,2), moving to the left and upward, the state value will be -1.7; while moving to right and downward, the state value will be -2.0; Therefore, the updated policy (to estimate k=3 State Value) for position (2,2) should be only move left (50%) or upward (50%), and not moving right or downward because they have less state value\n",
    "<img src='pic/policy_improve_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "* 老贾掌管两个租车行A,B，有车的时候只要有单，就可赚10元。老贾每晚可以在两个租车行间调配第二天的备用车辆数量。每移动一辆，花费2元。已知每天租车和还车的客人成泊松分布$\\frac{e^{-\\lambda}\\lambda^{n}}{n!}$。对每个车行，每天来租车和还车的泊松分布参数分别为租车A,B = 3,4, 还车A,B = 3,2. 每个车行最多有20辆后备车。$\\gamma$=0.9，求每晚老贾在A,B间调配车辆的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/policy_improve_2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 为什么持续迭代更新旧的策略可以获得更好的策略？\n",
    "* 命题：如果$q_{\\pi}(s,\\pi'(s))>=v_{\\pi}(s)$，那么$v_{\\pi'}(s)>=v_{\\pi}(s)$\n",
    "    - Prove as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/policy_improve_3.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
