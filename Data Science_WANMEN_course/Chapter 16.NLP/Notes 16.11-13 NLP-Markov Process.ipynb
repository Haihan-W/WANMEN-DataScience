{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba # 分词软件\n",
    "import re #normalization \n",
    "import copy \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《红楼梦》曹雪芹\n",
      "\n",
      "\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "\n",
      "在线阅读：http://www.shuyaya.com/read/2034/\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "第一回  甄士隐梦幻识通灵　贾雨村风尘怀闺秀\n",
      "\n",
      "\n",
      "\n",
      "    \t\t\n",
      "\n",
      "\n",
      "\n",
      "    此开卷第一回也．作者自云：因曾历过一番梦幻之后，故将真事隐去，而借\"通灵\"之说，撰此《石头记》一书也．故曰\"甄士隐\"云云．但书中所记何事何人？自又云：“今风尘碌碌，一事无成，忽念及当日所有之女子，一一细考较去，觉其行止见识，皆出于我之上．何我堂堂须眉，诚不若彼裙钗哉？实愧则有余，悔又无益之大无可如何之日也！当此，则自欲将已往所赖天恩祖德，锦衣纨绔之时，饫甘餍肥之日，背父兄教育之恩，负师友规谈之德，以至今日一技无成，半生潦倒之罪，编述一集，以告天下人：我之罪固不免，然闺阁中本自历历有人，万不可因我之不肖，自护己短，一并使其泯灭也．虽今日之茅椽蓬牖，瓦灶绳床，其晨夕风露，阶柳庭花，亦未有妨我之襟怀笔墨者．虽我未学，下笔无文，又何妨用假语村言，敷演出一段故事来，亦可使闺阁昭传，复可悦世之目，破人愁闷，不亦宜乎？\"故曰\"贾雨村\"云云．\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/hongloumeng.txt', encoding='utf-8', mode = 'r') as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        print (line)\n",
    "        i+=1\n",
    "        if i>10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_sentence(text):  #cut sentences by punctuations\n",
    "    start = 0\n",
    "    i = 0\n",
    "    sentences = []\n",
    "\n",
    "    #punt_list = ',.!?:;~，。！？：；～'\n",
    "    #punt_list = ',.!?:;~，．。！？：；～'\n",
    "    punt_list = '.!?~． 。！？～\\n'\n",
    "    for word in text:\n",
    "        if word in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点\n",
    "            sentences.append(text[start:i+1])\n",
    "            start = i+1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            token = list(text[start:i+2]).pop() # 取下一个字符\n",
    "    if start < len(text):\n",
    "        sentences.append(text[start:])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('data/hongloumeng.txt', encoding='utf-8', mode = 'r')\n",
    "context=f.read()\n",
    "sentences=cut_sentence(context)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《红楼梦》曹雪芹\n",
      "\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.\n",
      "shuyaya.\n",
      "com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "\n",
      "在线阅读：http://www.\n",
      "shuyaya.\n",
      "com/read/2034/\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "第一回  \n",
      "甄士隐梦幻识通灵　贾雨村风尘怀闺秀\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for s in sentences[:10]:\n",
    "    print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_re = re.compile(u'[^\\u4E00-\\u9FA5]') # all chinese characters' encoding range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('data/cleaned_hongloumeng.txt', encoding='utf-8', mode = 'w') #created a new writable/editable doc called \"cleaned_hongloumeng.txt'\n",
    "for s in sentences:\n",
    "    cleaned_s=filter_re.sub(r'',s) # substitute all non-chinese characters in sentences\n",
    "    if len(cleaned_s):\n",
    "        f.write(cleaned_s+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now all punctuations and english characters have been removed; Only Chinese characters have been kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put all chinese sentences into a list\n",
    "cleaned_sentences = []\n",
    "for s in sentences:\n",
    "    cleaned_s = filter_re.sub(r'', s)\n",
    "    if len(cleaned_s) :\n",
    "        cleaned_sentences.append(cleaned_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['红楼梦曹雪芹',\n",
       " '严正声明本书为丫丫小说网',\n",
       " '的用户上传至其在本站的存储空间本站只提供全集电子书存储服务以及免费下载服务以下作品内容之版权与本站无任何关系',\n",
       " '在线阅读',\n",
       " '第一回',\n",
       " '甄士隐梦幻识通灵贾雨村风尘怀闺秀',\n",
       " '此开卷第一回也',\n",
       " '作者自云因曾历过一番梦幻之后故将真事隐去而借通灵之说撰此石头记一书也',\n",
       " '故曰甄士隐云云',\n",
       " '但书中所记何事何人',\n",
       " '自又云今风尘碌碌一事无成忽念及当日所有之女子一一细考较去觉其行止见识皆出于我之上',\n",
       " '何我堂堂须眉诚不若彼裙钗哉',\n",
       " '实愧则有余悔又无益之大无可如何之日也',\n",
       " '当此则自欲将已往所赖天恩祖德锦衣纨绔之时饫甘餍肥之日背父兄教育之恩负师友规谈之德以至今日一技无成半生潦倒之罪编述一集以告天下人我之罪固不免然闺阁中本自历历有人万不可因我之不肖自护己短一并使其泯灭也',\n",
       " '虽今日之茅椽蓬牖瓦灶绳床其晨夕风露阶柳庭花亦未有妨我之襟怀笔墨者',\n",
       " '虽我未学下笔无文又何妨用假语村言敷演出一段故事来亦可使闺阁昭传复可悦世之目破人愁闷不亦宜乎',\n",
       " '故曰贾雨村云云',\n",
       " '此回中凡用梦用幻等字是提醒阅者眼目亦是此书立意本旨',\n",
       " '列位看官你道此书从何而来',\n",
       " '说起根由虽近荒唐细按则深有趣味']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use jieba.cut() to cut sentence to phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "说起\n",
      "根由\n",
      "虽近\n",
      "荒唐\n",
      "细\n",
      "按\n",
      "则\n",
      "深有\n",
      "趣味\n"
     ]
    }
   ],
   "source": [
    "# an example:\n",
    "for s in jieba.cut('说起根由虽近荒唐细按则深有趣味'):\n",
    "    print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jieba.cut('说起根由虽近荒唐细按则深有趣味'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x0000013A6187AA40>\n"
     ]
    }
   ],
   "source": [
    "print (jieba.cut('说起根由虽近荒唐细按则深有趣味'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['说起', '根由', '虽近', '荒唐', '细', '按', '则', '深有', '趣味']\n"
     ]
    }
   ],
   "source": [
    "print (list(jieba.cut('说起根由虽近荒唐细按则深有趣味')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['红楼梦', '曹雪芹']\n"
     ]
    }
   ],
   "source": [
    "# Just like above, do the similar for all sentences in cleaned_sentences\n",
    "cut_sentences = [list(jieba.cut(s)) for s in cleaned_sentences]\n",
    "print (cut_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['红楼梦', '曹雪芹'],\n",
       " ['严正声明', '本书', '为', '丫丫', '小说网'],\n",
       " ['的',\n",
       "  '用户',\n",
       "  '上传',\n",
       "  '至',\n",
       "  '其',\n",
       "  '在',\n",
       "  '本站',\n",
       "  '的',\n",
       "  '存储空间',\n",
       "  '本站',\n",
       "  '只',\n",
       "  '提供',\n",
       "  '全集',\n",
       "  '电子书',\n",
       "  '存储',\n",
       "  '服务',\n",
       "  '以及',\n",
       "  '免费',\n",
       "  '下载',\n",
       "  '服务',\n",
       "  '以下',\n",
       "  '作品',\n",
       "  '内容',\n",
       "  '之',\n",
       "  '版权',\n",
       "  '与',\n",
       "  '本站',\n",
       "  '无',\n",
       "  '任何',\n",
       "  '关系'],\n",
       " ['在线', '阅读'],\n",
       " ['第一回'],\n",
       " ['甄士隐', '梦幻', '识通灵', '贾雨村', '风尘', '怀', '闺秀'],\n",
       " ['此', '开卷', '第一回', '也'],\n",
       " ['作者',\n",
       "  '自云因',\n",
       "  '曾',\n",
       "  '历过',\n",
       "  '一番',\n",
       "  '梦幻',\n",
       "  '之后',\n",
       "  '故',\n",
       "  '将',\n",
       "  '真事',\n",
       "  '隐去',\n",
       "  '而',\n",
       "  '借',\n",
       "  '通灵之',\n",
       "  '说',\n",
       "  '撰此',\n",
       "  '石头记',\n",
       "  '一书',\n",
       "  '也'],\n",
       " ['故曰', '甄士隐', '云云'],\n",
       " ['但书中', '所记', '何事', '何人']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model: n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_ngrams = lambda sentence, n : zip(*[sentence[i:] for i in range(n)]) # set up a function to generate n-grams language model\n",
    "# note: n in above argument --> n-gram (i.e. n=1, unigram; n=2, bigram; n=3, trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自\n",
      "又\n",
      "云今\n",
      "风尘碌碌\n",
      "一事无成\n",
      "忽\n",
      "念及\n",
      "当日\n",
      "所有\n",
      "之\n",
      "女子\n",
      "一一\n",
      "细考\n",
      "较\n",
      "去\n",
      "觉\n",
      "其\n",
      "行止\n",
      "见识\n",
      "皆\n",
      "出于\n",
      "我\n",
      "之上\n"
     ]
    }
   ],
   "source": [
    "for unigram in generate_ngrams(cut_sentences[10], 1):\n",
    "    print (unigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 又\n",
      "又 云今\n",
      "云今 风尘碌碌\n",
      "风尘碌碌 一事无成\n",
      "一事无成 忽\n",
      "忽 念及\n",
      "念及 当日\n",
      "当日 所有\n",
      "所有 之\n",
      "之 女子\n",
      "女子 一一\n",
      "一一 细考\n",
      "细考 较\n",
      "较 去\n",
      "去 觉\n",
      "觉 其\n",
      "其 行止\n",
      "行止 见识\n",
      "见识 皆\n",
      "皆 出于\n",
      "出于 我\n",
      "我 之上\n"
     ]
    }
   ],
   "source": [
    "# test generate_ngrams function\n",
    "for bigram in generate_ngrams(cut_sentences[10], 2):\n",
    "    print (bigram[0], bigram[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 又 云今\n",
      "又 云今 风尘碌碌\n",
      "云今 风尘碌碌 一事无成\n",
      "风尘碌碌 一事无成 忽\n",
      "一事无成 忽 念及\n",
      "忽 念及 当日\n",
      "念及 当日 所有\n",
      "当日 所有 之\n",
      "所有 之 女子\n",
      "之 女子 一一\n",
      "女子 一一 细考\n",
      "一一 细考 较\n",
      "细考 较 去\n",
      "较 去 觉\n",
      "去 觉 其\n",
      "觉 其 行止\n",
      "其 行止 见识\n",
      "行止 见识 皆\n",
      "见识 皆 出于\n",
      "皆 出于 我\n",
      "出于 我 之上\n"
     ]
    }
   ],
   "source": [
    "for trigram in generate_ngrams(cut_sentences[10], 3):\n",
    "    print (trigram[0],trigram[1],trigram[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likehood Parameter Estimation\n",
    "After setting-up n-gram model, next step is to calculate maximum likelihood parameter estimation (PDF P22, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximum likelihood parameter estimation \n",
    "def ngrams_parameter_estimate(sentences, n):\n",
    "    sentences_copy = copy.deepcopy(sentences)\n",
    "    ngrams_dict = {}\n",
    "    num_ngrams = 0\n",
    "    for words in sentences_copy:\n",
    "        for i in range(n-1):\n",
    "            words.insert(0, '*')  # add \"*\" at beginning of each cutted sentence\n",
    "        words.append('#')  # add stop sign \"#\" at end of each cutted sentence\n",
    "        ngrams = generate_ngrams(words, n) # sub those to pre-defined n-gram language model above\n",
    "        for ngram in ngrams:\n",
    "            ngrams_dict[ngram] = ngrams_dict.get(ngram, 0.0) + 1.0  # form a dictionary with key=word, value=frequency\n",
    "            num_ngrams += 1  # count total # of unique word appeared\n",
    "    \n",
    "    # normalize (optional)\n",
    "    #for ngram in ngrams_dict:\n",
    "        #ngrams_dict[ngram] /= num_ngrams\n",
    "        \n",
    "    return ngrams_dict, num_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example: unigram\n",
    "unigrams_dict, n_unigrams = ngrams_parameter_estimate(cut_sentences, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  ('红楼梦',)\n",
      "v:  9.0\n",
      "k:  ('曹雪芹',)\n",
      "v:  5.0\n",
      "k:  ('#',)\n",
      "v:  34412.0\n",
      "k:  ('严正声明',)\n",
      "v:  1.0\n",
      "k:  ('本书',)\n",
      "v:  7.0\n",
      "k:  ('为',)\n",
      "v:  529.0\n",
      "k:  ('丫丫',)\n",
      "v:  1.0\n",
      "k:  ('小说网',)\n",
      "v:  1.0\n",
      "k:  ('的',)\n",
      "v:  14115.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in unigrams_dict.items():\n",
    "    print (\"k: \",k)\n",
    "    print (\"v: \",v)\n",
    "    i+=1\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fx(k,v):\n",
    "    return (v,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by frequency of word, create a sorted_dictionary with key=(word,), value=frequency where frequency>=10 (note: we\n",
    "ignored words with frequency<10, i.e. v<10 as below)\n",
    "filtered_unigrams_dict = OrderedDict(sorted([(k, v) for k, v in unigrams_dict.items() if v >= 10], key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PLEASE NOTE: In Python 3, dictionaries (including OrderedDict) return \"view\" objects from their keys() and values() methods. Those are iterable, but don't support indexing. \n",
    "- To refer to position of the list of .key() or .value(), e.g. .keys()[0] (the first key in dictionary), you need to make list() of .keys() then refer to the position by indexing, e.g. list(dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listofkey=list(filtered_unigrams_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#',)\n",
      "34412.0\n"
     ]
    }
   ],
   "source": [
    "print(listofkey[0])  #the word of key in first position(highest frequency)\n",
    "print(filtered_unigrams_dict[listofkey[0]]) #its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: #, frequency: 34412.0\n",
      "word: 了, frequency: 19613.0\n",
      "word: 今儿, frequency: 286.0\n",
      "word: 查抄, frequency: 10.0\n"
     ]
    }
   ],
   "source": [
    "print ('word: %s, frequency: %s' % (listofkey[0][0], filtered_unigrams_dict[listofkey[0]])) # highest frequency\n",
    "print ('word: %s, frequency: %s' % (listofkey[1][0], filtered_unigrams_dict[listofkey[1]])) # 2nd highest frequency\n",
    "print ('word: %s, frequency: %s' % (listofkey[200][0], filtered_unigrams_dict[listofkey[200]])) # 201 highest frequency\n",
    "print ('word: %s, frequency: %s' % (listofkey[-1][0], filtered_unigrams_dict[listofkey[-1]])) # the least frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PLEASE NOTE: similar like .keys(), dict.values() in python 3 still needs list() to actually use it instead of viewing it (see below list(filtered_unigrams_dict.values()) [note: np.log10(filtered_unigrams_dict.values()) won't work, it only worked in python 2, I added a list() to make it work in python 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0.5,'frequency'), Text(0.5,0,'log(word frequency)')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a6d753630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.distplot(np.log10(list(filtered_unigrams_dict.values())))\n",
    "ax.set(xlabel='log(word frequency)', ylabel='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Language Model with Perplexity(迷惑度) with train/test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- see PDF P 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(cut_sentences)\n",
    "n_valid = int(n_samples * 0.2)\n",
    "n_test = int(n_samples * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_sentences = np.array(cut_sentences)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = shuffled_sentences[:n_samples-n_test-n_valid]\n",
    "valid = shuffled_sentences[n_samples-n_test-n_valid:n_samples-n_test]\n",
    "test = shuffled_sentences[n_samples-n_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Language Model on Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_dict, n_unigrams = ngrams_parameter_estimate(train, 1)\n",
    "bigrams_dict, n_bigrams = ngrams_parameter_estimate(train, 2)\n",
    "trigrams_dict, n_trigrams = ngrams_parameter_estimate(train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242714"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242714"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242714"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why same amount of # grams among the above three? -- Because I have added a \"*\" and \"#\" sign for each cutted sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Our Uni-, Bi-, Tri-gram Model on Testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Testing using general log probability to calculate perplexity (PDF P23-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentence_log_probability(sentence, n, num_grams, ngrams_dict, n_1grams_dict=None): #log probability (PDF P23)\n",
    "    assert len(list(ngrams_dict.keys())[0])== n, n\n",
    "    if n_1grams_dict is None:\n",
    "        assert n == 1\n",
    "    ngrams = generate_ngrams(sentence, n)\n",
    "    log_prob, has_unknown_ngram = 0.0, False\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngrams_dict:\n",
    "            if n == 1:\n",
    "                log_prob += np.log2(ngrams_dict[ngram]/num_grams)\n",
    "            else:\n",
    "                log_prob += np.log2(ngrams_dict[ngram]/n_1grams_dict[ngram[:n-1]])\n",
    "        else:\n",
    "            has_unknown_ngram = True\n",
    "    return 0.0 if has_unknown_ngram else log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_total_words(sentences):\n",
    "    \n",
    "    return np.sum([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134818"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words_in_test = compute_total_words(test)\n",
    "n_words_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in test: 134818\n",
      "Perplexity for uni gram language model: 15.243750590959857\n",
      "Perplexity for bi gram language model: 1.0866422336263726\n",
      "Perplexity for tri gram language model: 1.0068247873077378\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity for n-grams model using testing set: (PDF P 23)\n",
    "ngrams_list = [unigram_dict, bigrams_dict, trigrams_dict]\n",
    "ngrams_names = ['uni', 'bi', 'tri']\n",
    "print (\"Number of words in test: {}\".format(n_words_in_test))\n",
    "for n in range(1, 4):\n",
    "    sum_log_prob = 0.0\n",
    "    for s in test:\n",
    "        if n == 1:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1])\n",
    "        else:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1], ngrams_list[n-2])\n",
    "    perplexity = 2**(-sum_log_prob/n_words_in_test)\n",
    "    \n",
    "    print (\"Perplexity for {} gram language model: {}\".format(ngrams_names[n-1], perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion for above: perplexity smaller means language is better(PDF P24)--> we can see unigram is the worst; bi-gram and trigram are both good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same testing using validation dataset, same as above, so we can skip for now...\n",
    "n_words_in_valid = compute_total_words(valid)\n",
    "print \"Number of words in valid: {}\".format(n_words_in_valid)\n",
    "for n in range(1, 4):\n",
    "    sum_log_prob = 0.0\n",
    "    for s in valid:\n",
    "        if n == 1:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1])\n",
    "        else:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1], ngrams_list[n-2])\n",
    "    perplexity = 2**(-sum_log_prob/n_words_in_valid)\n",
    "    \n",
    "    print \"Perplexity for {} gram language model: {}\".format(ngrams_names[n-1], perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Testing using log probability w/ Linear Interpolation Smoothing to calculate perplexity (PDF P26-27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_log_probability_with_linear_interpolation(sentence, num_grams, ngrams_list, lambdas):\n",
    "    ngrams = generate_ngrams(sentence, 3)\n",
    "    log_prob, has_unknown_ngram = 0.0, False\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngrams_list[2]:\n",
    "            prob = 0.0\n",
    "            for i in range(3):\n",
    "                if i == 0:\n",
    "                    prob += ngrams_list[i][ngram[:i+1]]*lambdas[i]/num_grams\n",
    "                else:\n",
    "                    prob += ngrams_list[i][ngram[:i+1]]*lambdas[i]/ngrams_list[i-1][ngram[:i]]\n",
    "            log_prob += np.log2(prob) if prob > 0.0 else 0.0\n",
    "        else:\n",
    "            has_unknown_ngram = True\n",
    "    return 0.0 if has_unknown_ngram else log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Find best lambdas combination below to Maximize Weighted Sum of Log Probability w/ Linear Interpolation Smoothing (Equation L(...)=... on PDF P 27) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_lambdas(sentences, n_iter, n_unigrams, ngrams_list, verbose=True):\n",
    "    best_lambda_1, best_lambda_2, best_lambda_3 = None, None, None\n",
    "    best_log_prob = -np.inf\n",
    "    for i in range(n_iter):\n",
    "        lambda_1 = np.random.uniform(0, 1)\n",
    "        lambda_2 = np.random.uniform(0, 1-lambda_1)\n",
    "        lambda_3 = 1.0 - lambda_1 - lambda_2        # All three lambdas must be >0\n",
    "        lambdas = [lambda_1, lambda_2, lambda_3]\n",
    "        sum_log_prob = 0.0\n",
    "        for s in valid:\n",
    "            sum_log_prob += sentence_log_probability_with_linear_interpolation(s, n_unigrams, ngrams_list, lambdas)\n",
    "        if sum_log_prob > best_log_prob:\n",
    "            best_log_prob = sum_log_prob\n",
    "            best_lambda_1, best_lambda_2, best_lambda_3 = tuple(lambdas)\n",
    "        if verbose:\n",
    "            print (\"current best log_prob: {}\".format(best_log_prob))\n",
    "            \n",
    "    return [best_lambda_1, best_lambda_2, best_lambda_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best log_prob: -1321.9976076970513\n",
      "current best log_prob: -1178.2458477273863\n",
      "current best log_prob: -1178.2458477273863\n",
      "current best log_prob: -1178.2458477273863\n",
      "current best log_prob: -835.9468839964425\n",
      "current best log_prob: -835.9468839964425\n",
      "current best log_prob: -835.9468839964425\n",
      "current best log_prob: -835.9468839964425\n",
      "current best log_prob: -835.9468839964425\n",
      "current best log_prob: -835.9468839964425\n"
     ]
    }
   ],
   "source": [
    "best_lambdas = random_search_lambdas(valid, 10, n_unigrams, ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03449309798939648, 0.45996695823350303, 0.5055399437771004]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in valid: 89866\n",
      "Perplexity for smoothed trigram language model: 1.0064685895992813\n"
     ]
    }
   ],
   "source": [
    "# use best_lambdas to recalculate perplexity\n",
    "n_words_in_valid = compute_total_words(valid)\n",
    "lambdas = best_lambdas\n",
    "print (\"Number of words in valid: {}\".format(n_words_in_valid))\n",
    "sum_log_prob = 0.0\n",
    "for s in valid:\n",
    "    sum_log_prob += sentence_log_probability_with_linear_interpolation(s, n_unigrams, ngrams_list, lambdas)\n",
    "perplexity = 2**(-sum_log_prob/n_words_in_valid)\n",
    "    \n",
    "print (\"Perplexity for smoothed trigram language model: {}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion for method above with linear interpolation smoothing--> compared trigram model perplexity between this smoothing method (1.0064685895992813) and general method(without smoothing, 1.0068247873077378), we found results was improved a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Testing using log probability w/ Laplace Smoothing to calculate perplexity (PDF P28)- homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
